# NLP Fundamentals - Basic Concepts

**Learning Track**: Natural Language Processing  
**Learning Level**: Beginner  
**Domain**: Text Processing Foundations

## ğŸ“š Learning Progression

This directory contains foundational NLP concepts that every practitioner should understand before diving into advanced topics like transformers and LLMs.

### ğŸ¯ Recommended Learning Path

```text
Text Representation Journey:
01_One-Hot-Encoding.md
    â†“ (understand basic textâ†’numbers)
02_Tokenization-Basics.md  
    â†“ (learn text splitting fundamentals)
03_Bag-of-Words.md
    â†“ (simple counting approaches)
04_TF-IDF.md
    â†“ (weighted text representation)
05_Model-Tokenizer-Compatibility.md â† NEW!
    â†“ (critical for modern LLMs)
Advanced Text Representations â†’
```

### ğŸ“‹ Topic Overview

#### **01_One-Hot-Encoding**

- **What**: Basic binary text representation
- **Why Important**: Foundation for understanding how computers process text
- **Learning Level**: Beginner
- **Time**: 15 minutes

#### **02_Tokenization-Basics**

- **What**: Breaking text into processable units
- **Why Important**: Essential preprocessing step for all NLP
- **Learning Level**: Beginner  
- **Time**: 20 minutes

#### **03_Bag-of-Words**

- **What**: Simple word counting representation
- **Why Important**: Introduces concept of text as numerical features
- **Learning Level**: Beginner
- **Time**: 25 minutes

#### **04_TF-IDF**

- **What**: Weighted text representation considering word importance
- **Why Important**: Bridge between simple counting and modern embeddings
- **Learning Level**: Beginner to Intermediate
- **Time**: 30 minutes

#### **05_Model-Tokenizer-Compatibility** â­ **Essential for LLM Work**

- **What**: Understanding why different models need specific tokenizers
- **Why Important**: Prevents critical errors when working with modern LLMs
- **Learning Level**: Beginner to Intermediate
- **Time**: 20 minutes

## ğŸ”— Connections to Advanced Topics

### Enables Understanding Of

- [Tokenization and Token IDs](../03_Tokenization-and-Token-IDs.md)
- [Word Embeddings](../02_Text-Representation/04_Word2Vec-and-Embeddings.md)
- [LLM Fundamentals](../../05_LargeLanguageModels/01_LLM-Fundamentals.md)
- [Transformer Architecture](../../03_DeepLearning/01_Transformer-Architecture.md)

### Prerequisites From Other Tracks

- Basic Python programming
- Understanding of vectors and matrices (linear algebra basics)
- Familiarity with machine learning concepts

## ğŸ’¡ Key Learning Outcomes

After completing this basics section, you will:

- âœ… Understand how text is converted to numerical representations
- âœ… Know when to use different text representation methods
- âœ… Avoid critical tokenizer-model compatibility errors
- âœ… Be prepared for advanced NLP and LLM topics
- âœ… Understand the evolution from simple counting to modern embeddings

## ğŸ¯ Next Steps

Once you've mastered these basics:

1. **Advanced Text Representations**: Move to `../02_Text-Representation/`
2. **Practical Applications**: Explore `../05_Text-Classification.md`
3. **Modern Approaches**: Study LLM and Transformer topics
4. **Hands-On Practice**: Try the examples in the `examples/` directory

---

**Last Updated**: September 2025  
**Review Schedule**: Quarterly with NLP track updates
