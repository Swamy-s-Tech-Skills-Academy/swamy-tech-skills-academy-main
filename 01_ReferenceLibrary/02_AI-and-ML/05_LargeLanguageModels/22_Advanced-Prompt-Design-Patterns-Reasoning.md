# Advanced Prompt Design Patterns - Part C: Few-Shot Learning & Complex Reasoning

**Learning Level**: Advanced  
**Prerequisites**: Parts A & B - Structural Foundations and Performance Optimization  
**Estimated Time**: 27 minutes  
**Part**: C of 3-part series on Advanced Prompt Design

---

## ðŸŽ¯ Learning Objectives

By the end of this 27-minute session, you will:

- **Master few-shot learning techniques** for complex problem-solving
- **Design reasoning chains** for multi-step analysis
- **Implement dynamic example selection** for optimal learning
- **Create production-ready inference patterns** for enterprise applications

---

## ðŸ§  Few-Shot Learning Mastery

### **The Science of Example-Driven Learning**

Few-shot learning leverages the model's pattern recognition to learn from examples rather than explicit instructions. Strategic example selection can dramatically improve output quality and consistency.

### **Example Architecture Patterns**

```python
class FewShotExampleManager:
    """Manage and optimize few-shot learning examples."""
    
    def __init__(self):
        self.example_bank = {}
        self.performance_metrics = {}
    
    def add_example_set(self, task_type: str, examples: list, 
                       performance_score: float = None):
        """Add curated example set for specific task type."""
        self.example_bank[task_type] = {
            "examples": examples,
            "performance_score": performance_score,
            "usage_count": 0,
            "created_date": datetime.now()
        }
    
    def get_optimal_examples(self, task_type: str, context_length: int = 2000) -> list:
        """Select optimal examples within context constraints."""
        if task_type not in self.example_bank:
            return []
        
        examples = self.example_bank[task_type]["examples"]
        
        # Estimate token usage and select examples that fit
        selected_examples = []
        current_tokens = 0
        
        for example in examples:
            example_tokens = len(str(example).split()) * 1.3  # Rough estimation
            if current_tokens + example_tokens <= context_length:
                selected_examples.append(example)
                current_tokens += example_tokens
            else:
                break
        
        # Track usage
        self.example_bank[task_type]["usage_count"] += 1
        
        return selected_examples

# Task-specific example sets
security_audit_examples = [
    {
        "input": "function validateUser(input) { return eval(input.username); }",
        "analysis": {
            "vulnerability": "Code Injection (CWE-94)",
            "severity": "CRITICAL", 
            "location": "eval() function call",
            "impact": "Remote code execution, full system compromise",
            "remediation": "Replace eval() with JSON.parse() and input validation"
        }
    },
    {
        "input": "SELECT * FROM users WHERE id = '" + userId + "'",
        "analysis": {
            "vulnerability": "SQL Injection (CWE-89)",
            "severity": "HIGH",
            "location": "String concatenation in SQL query",
            "impact": "Data breach, unauthorized access", 
            "remediation": "Use parameterized queries or prepared statements"
        }
    },
    {
        "input": "app.get('/api/file', (req, res) => { res.sendFile(req.query.path); })",
        "analysis": {
            "vulnerability": "Path Traversal (CWE-22)",
            "severity": "HIGH",
            "location": "Direct file path from user input",
            "impact": "Arbitrary file access, information disclosure",
            "remediation": "Validate and sanitize file paths, use whitelist approach"
        }
    }
]

performance_optimization_examples = [
    {
        "scenario": "Database query taking 3+ seconds",
        "code": "SELECT * FROM orders o JOIN customers c ON o.customer_id = c.id WHERE o.created_at > '2024-01-01'",
        "analysis": {
            "bottleneck": "Full table scan on orders table",
            "solution": "Add composite index on (created_at, customer_id)",
            "expected_improvement": "95% reduction in query time",
            "implementation": "CREATE INDEX idx_orders_date_customer ON orders(created_at, customer_id);"
        }
    },
    {
        "scenario": "API response time degradation under load",
        "code": "async function getProductRecommendations(userId) { const user = await User.findById(userId); const purchases = await Purchase.findAll({where: {userId}}); return analyzeRecommendations(purchases); }",
        "analysis": {
            "bottleneck": "N+1 query problem with sequential database calls",
            "solution": "Implement batch loading with single query",
            "expected_improvement": "80% reduction in database round trips",
            "implementation": "Use JOIN or include relations in single query"
        }
    }
]
```

### **Dynamic Example Selection**

```text
ROLE: Senior DevOps Engineer

TASK: Infrastructure problem diagnosis and resolution

EXAMPLE SELECTION STRATEGY:
- Choose examples that match current problem domain
- Prioritize recent, successful resolution patterns
- Include edge cases relevant to current context
- Balance complexity to match required detail level

EXAMPLE 1 - Database Performance Issue:
PROBLEM: PostgreSQL queries timing out during peak traffic
INVESTIGATION: Analyzed slow query logs, identified missing indexes on frequently queried columns
SOLUTION: Created composite indexes on high-cardinality columns, implemented query optimization
RESULT: 85% reduction in query time, eliminated timeouts
TOOLS: pg_stat_statements, EXPLAIN ANALYZE, pgBadger

EXAMPLE 2 - Container Memory Issues:
PROBLEM: Kubernetes pods experiencing OOM kills under moderate load
INVESTIGATION: Memory profiling revealed memory leaks in application code and misconfigured resource limits
SOLUTION: Fixed application memory leaks, optimized container resource requests/limits, implemented horizontal pod autoscaling
RESULT: Zero OOM kills, 40% better resource utilization
TOOLS: kubectl top, Prometheus metrics, application profiling

EXAMPLE 3 - Network Latency Spikes:
PROBLEM: Intermittent 5-second delays in API responses
INVESTIGATION: Network monitoring revealed DNS resolution issues and connection pooling problems
SOLUTION: Implemented local DNS caching, optimized connection pool configuration, added circuit breakers
RESULT: 95% reduction in latency spikes, improved error handling
TOOLS: tcpdump, dig, application performance monitoring

CURRENT PROBLEM CONTEXT: {specific_infrastructure_issue}

ANALYSIS PATTERN: Follow investigation â†’ solution â†’ result â†’ tools pattern from examples above.
```

---

## ðŸ”— Chain-of-Thought Reasoning

### **Structured Reasoning Frameworks**

```text
ROLE: Software Architecture Consultant

REASONING FRAMEWORK: Progressive Analysis Pattern

STEP 1 - PROBLEM DECOMPOSITION:
Break down complex architecture challenges into analyzable components
- Identify core functional requirements
- Map non-functional requirements (performance, scalability, security)
- Determine integration constraints and dependencies
- Assess risk factors and compliance requirements

STEP 2 - PATTERN MATCHING:
Compare current scenario with established architectural patterns
- Evaluate microservices vs monolithic approaches
- Consider event-driven vs request-response architectures  
- Assess data consistency requirements (ACID vs eventual consistency)
- Match to proven patterns: CQRS, Event Sourcing, Saga, Circuit Breaker

STEP 3 - CONSTRAINT ANALYSIS:
Evaluate technical and business constraints
- Team expertise and learning curve considerations
- Technology stack compatibility and vendor lock-in
- Budget and timeline constraints
- Operational complexity and maintenance overhead

STEP 4 - SOLUTION SYNTHESIS:
Combine patterns and constraints into coherent architecture
- Design component interaction patterns
- Define data flow and state management strategies
- Specify deployment and scaling strategies
- Plan monitoring and observability approaches

STEP 5 - RISK ASSESSMENT:
Identify potential failure modes and mitigation strategies
- Single points of failure analysis
- Performance bottleneck identification
- Security vulnerability assessment
- Operational complexity evaluation

STEP 6 - IMPLEMENTATION ROADMAP:
Create phased implementation plan with measurable milestones
- Define MVP functionality and incremental feature additions
- Specify technical spikes and proof-of-concept requirements
- Identify critical decision points and fallback options
- Plan for monitoring and feedback loops

ARCHITECTURAL CHALLENGE: {complex_architecture_problem}

APPLY REASONING FRAMEWORK: Work through each step systematically, providing detailed analysis for each phase.
```

### **Multi-Step Problem Solving**

```python
class ChainOfThoughtPromptBuilder:
    """Build structured reasoning prompts for complex problems."""
    
    def __init__(self):
        self.reasoning_steps = []
        self.context_variables = {}
    
    def add_reasoning_step(self, step_name: str, analysis_points: list, 
                          output_format: str = None) -> 'ChainOfThoughtPromptBuilder':
        """Add a reasoning step to the chain."""
        step = {
            "name": step_name,
            "analysis_points": analysis_points,
            "output_format": output_format or "Structured analysis with specific findings"
        }
        self.reasoning_steps.append(step)
        return self
    
    def set_context(self, **kwargs) -> 'ChainOfThoughtPromptBuilder':
        """Set context variables for the reasoning chain."""
        self.context_variables.update(kwargs)
        return self
    
    def build_prompt(self, role: str, problem_description: str) -> str:
        """Generate the complete chain-of-thought prompt."""
        prompt = f"""
ROLE: {role}

PROBLEM: {problem_description}

REASONING METHODOLOGY: Progressive Analysis Chain

"""
        
        for i, step in enumerate(self.reasoning_steps, 1):
            prompt += f"""
STEP {i} - {step['name'].upper()}:
{chr(10).join(f"- {point}" for point in step['analysis_points'])}

OUTPUT REQUIREMENT: {step['output_format']}

"""
        
        if self.context_variables:
            prompt += "\nCONTEXT VARIABLES:\n"
            for key, value in self.context_variables.items():
                prompt += f"- {key}: {value}\n"
        
        prompt += """
EXECUTION INSTRUCTION: Work through each step systematically. Complete each step's analysis before proceeding to the next. Show your reasoning process clearly.
"""
        
        return prompt

# Example usage for security architecture analysis
security_reasoning_prompt = (ChainOfThoughtPromptBuilder()
    .add_reasoning_step(
        "Threat Landscape Analysis",
        [
            "Identify potential attack vectors for the specific system type",
            "Assess threat actor capabilities and motivations", 
            "Evaluate current security posture and gaps",
            "Map to MITRE ATT&CK framework tactics and techniques"
        ],
        "Threat assessment matrix with risk ratings"
    )
    .add_reasoning_step(
        "Security Control Evaluation", 
        [
            "Review existing security controls effectiveness",
            "Identify control gaps and redundancies",
            "Assess defense-in-depth implementation",
            "Evaluate monitoring and detection capabilities"
        ],
        "Control effectiveness scorecard with improvement recommendations"
    )
    .add_reasoning_step(
        "Risk Prioritization",
        [
            "Calculate risk scores based on likelihood and impact",
            "Consider business context and critical asset protection",
            "Evaluate cost-benefit of potential security improvements",
            "Align with regulatory and compliance requirements"
        ],
        "Prioritized risk register with mitigation strategies"
    )
    .set_context(
        system_type="E-commerce platform with payment processing",
        user_base="100K+ active users",
        compliance_requirements="PCI-DSS, GDPR",
        current_architecture="Microservices on AWS with API Gateway"
    )
    .build_prompt(
        role="Senior Security Architect",
        problem_description="Design comprehensive security architecture for high-traffic e-commerce platform handling sensitive payment data"
    ))
```

---

## ðŸŽ¯ Advanced Reasoning Patterns

### **Comparative Analysis Framework**

```text
ROLE: Technology Evaluation Specialist

METHODOLOGY: Multi-Criteria Decision Analysis (MCDA)

EVALUATION FRAMEWORK: Systematic Technology Comparison

COMPARISON CRITERIA:
1. Technical Capabilities
   - Performance characteristics (throughput, latency, scalability)
   - Feature completeness and extensibility
   - Integration ecosystem and API quality
   - Development experience and tooling

2. Operational Considerations  
   - Deployment complexity and infrastructure requirements
   - Monitoring and debugging capabilities
   - Resource consumption (CPU, memory, network)
   - Maintenance overhead and update frequency

3. Business Factors
   - Total cost of ownership (licensing, infrastructure, development)
   - Vendor stability and long-term viability  
   - Community support and ecosystem maturity
   - Learning curve and team expertise requirements

4. Risk Assessment
   - Technology maturity and production readiness
   - Vendor lock-in potential and exit strategies
   - Security track record and vulnerability management
   - Compliance and regulatory considerations

SCORING METHODOLOGY:
- Rate each criterion on 1-10 scale with clear justification
- Apply weighted importance multipliers based on business priorities
- Calculate composite scores with confidence intervals
- Identify sensitivity to assumption changes

COMPARISON TARGET: {technology_options}

BUSINESS CONTEXT: {specific_business_requirements}

ANALYSIS INSTRUCTION: Evaluate each technology systematically against all criteria. Provide specific evidence for ratings and clear recommendation with risk factors.
```

### **Root Cause Analysis Pattern**

```text
ROLE: Senior System Reliability Engineer

INCIDENT ANALYSIS METHODOLOGY: Systematic Root Cause Analysis

ANALYSIS FRAMEWORK: Five Whys + Fishbone + Timeline

INCIDENT TIMELINE RECONSTRUCTION:
1. Establish clear incident timeline with specific timestamps
2. Identify first observable symptoms and escalation points
3. Map correlation with deployments, configuration changes, external events
4. Document all investigative actions and findings during incident response

ROOT CAUSE EXPLORATION:
Why 1: What was the immediate trigger for the incident?
- Direct cause analysis with supporting evidence
- Distinguish between symptoms and actual triggers

Why 2: What underlying condition allowed this trigger to cause failure?
- System vulnerability or design limitation analysis
- Configuration or operational state contributing factors

Why 3: What process or design decisions led to this underlying condition?
- Architectural decisions, process gaps, or human factors
- Historical context and evolution of the problematic state

Why 4: What organizational or systematic factors influenced these decisions?
- Resource constraints, knowledge gaps, or conflicting priorities
- Team structure, communication patterns, or tooling limitations

Why 5: What fundamental system or process improvements would prevent recurrence?
- Systemic changes to technology, process, or organizational structure
- Long-term strategic improvements beyond immediate fixes

CONTRIBUTING FACTOR ANALYSIS (Fishbone Categories):
- Technology: Hardware, software, infrastructure, architecture
- Process: Procedures, workflows, communication, documentation  
- People: Skills, training, workload, decision-making
- Environment: External dependencies, timing, market conditions

INCIDENT CONTEXT: {specific_incident_description}

AVAILABLE DATA: {monitoring_logs_metrics_evidence}

ANALYSIS REQUIREMENT: Provide systematic analysis through each phase with specific evidence and actionable prevention strategies.
```

---

## ðŸ”„ Dynamic Prompt Adaptation

### **Context-Aware Prompt Evolution**

```python
class AdaptivePromptSystem:
    """Dynamically adapt prompts based on context and performance."""
    
    def __init__(self):
        self.base_prompts = {}
        self.adaptation_rules = {}
        self.performance_history = {}
    
    def register_base_prompt(self, prompt_id: str, template: str, 
                           adaptation_triggers: dict):
        """Register a base prompt with adaptation rules."""
        self.base_prompts[prompt_id] = {
            "template": template,
            "adaptation_triggers": adaptation_triggers,
            "usage_count": 0,
            "avg_quality": 0.0
        }
    
    def add_adaptation_rule(self, prompt_id: str, condition: str, 
                          modification: str, priority: int = 1):
        """Add adaptation rule for specific conditions."""
        if prompt_id not in self.adaptation_rules:
            self.adaptation_rules[prompt_id] = []
        
        self.adaptation_rules[prompt_id].append({
            "condition": condition,
            "modification": modification,
            "priority": priority,
            "success_rate": 0.0
        })
    
    def generate_adapted_prompt(self, prompt_id: str, context: dict) -> str:
        """Generate adapted prompt based on current context."""
        base_prompt = self.base_prompts[prompt_id]["template"]
        
        # Apply adaptation rules based on context
        adapted_prompt = base_prompt
        applied_rules = []
        
        if prompt_id in self.adaptation_rules:
            # Sort rules by priority and success rate
            rules = sorted(
                self.adaptation_rules[prompt_id],
                key=lambda x: (x["priority"], x["success_rate"]),
                reverse=True
            )
            
            for rule in rules:
                if self._evaluate_condition(rule["condition"], context):
                    adapted_prompt = self._apply_modification(
                        adapted_prompt, rule["modification"], context
                    )
                    applied_rules.append(rule)
        
        # Format with context variables
        try:
            adapted_prompt = adapted_prompt.format(**context)
        except KeyError as e:
            print(f"Warning: Missing context variable {e}")
        
        return adapted_prompt
    
    def _evaluate_condition(self, condition: str, context: dict) -> bool:
        """Evaluate if adaptation condition is met."""
        # Simple condition evaluation (extend for complex logic)
        condition_mappings = {
            "high_complexity": context.get("complexity_score", 0) > 7,
            "performance_critical": context.get("performance_requirement") == "critical",
            "security_sensitive": context.get("security_level") in ["high", "critical"],
            "large_codebase": context.get("codebase_size", 0) > 100000,
            "team_inexperienced": context.get("team_experience") == "junior"
        }
        
        return condition_mappings.get(condition, False)
    
    def _apply_modification(self, prompt: str, modification: str, context: dict) -> str:
        """Apply specific modification to prompt."""
        modification_patterns = {
            "add_examples": lambda p: p + "\n\nEXAMPLES:\n{examples}",
            "increase_detail": lambda p: p.replace("analysis", "detailed step-by-step analysis"),
            "add_security_focus": lambda p: p + "\n\nSECURITY EMPHASIS: Pay special attention to security implications and vulnerabilities.",
            "simplify_language": lambda p: p.replace("sophisticated", "clear").replace("comprehensive", "complete"),
            "add_performance_focus": lambda p: p + "\n\nPERFORMANCE PRIORITY: Optimize for speed and efficiency in all recommendations."
        }
        
        if modification in modification_patterns:
            return modification_patterns[modification](prompt)
        
        return prompt

# Example adaptation rules
adaptive_system = AdaptivePromptSystem()

adaptive_system.register_base_prompt(
    "code_review",
    """
ROLE: {reviewer_role}
TASK: Review the following code for quality, security, and performance
CODE: {code_content}
FOCUS: {review_focus}
OUTPUT: Structured review with specific recommendations
""",
    {
        "complexity_score": "add_examples",
        "security_level": "add_security_focus", 
        "performance_requirement": "add_performance_focus"
    }
)

# Add specific adaptation rules
adaptive_system.add_adaptation_rule(
    "code_review",
    "security_sensitive",
    "add_security_focus",
    priority=10
)

adaptive_system.add_adaptation_rule(
    "code_review", 
    "team_inexperienced",
    "add_examples",
    priority=8
)

# Generate adapted prompt
context = {
    "reviewer_role": "Senior Security Engineer",
    "code_content": "authentication_module.py",
    "review_focus": "Security vulnerabilities",
    "security_level": "critical",
    "team_experience": "junior",
    "complexity_score": 8
}

adapted_prompt = adaptive_system.generate_adapted_prompt("code_review", context)
```

### **Learning from Feedback Loops**

```python
class FeedbackLearningSystem:
    """Learn and improve prompts based on outcome feedback."""
    
    def __init__(self):
        self.prompt_performance = {}
        self.feedback_patterns = {}
        self.improvement_suggestions = {}
    
    def record_outcome(self, prompt_id: str, prompt_text: str, 
                      output_quality: float, user_feedback: str = None,
                      metrics: dict = None):
        """Record prompt performance and feedback."""
        if prompt_id not in self.prompt_performance:
            self.prompt_performance[prompt_id] = []
        
        outcome_record = {
            "prompt_text": prompt_text,
            "quality_score": output_quality,
            "user_feedback": user_feedback,
            "metrics": metrics or {},
            "timestamp": datetime.now()
        }
        
        self.prompt_performance[prompt_id].append(outcome_record)
        
        # Analyze patterns if we have enough data
        if len(self.prompt_performance[prompt_id]) >= 10:
            self._analyze_feedback_patterns(prompt_id)
    
    def _analyze_feedback_patterns(self, prompt_id: str):
        """Identify patterns in prompt performance."""
        records = self.prompt_performance[prompt_id]
        
        # Find high-performing prompt variants
        high_quality = [r for r in records if r["quality_score"] >= 8.0]
        low_quality = [r for r in records if r["quality_score"] <= 5.0]
        
        if high_quality and low_quality:
            # Extract common patterns in high-performing prompts
            high_patterns = self._extract_prompt_patterns(
                [r["prompt_text"] for r in high_quality]
            )
            low_patterns = self._extract_prompt_patterns(
                [r["prompt_text"] for r in low_quality]
            )
            
            # Identify differentiating factors
            success_factors = set(high_patterns) - set(low_patterns)
            failure_factors = set(low_patterns) - set(high_patterns)
            
            self.feedback_patterns[prompt_id] = {
                "success_factors": list(success_factors),
                "failure_factors": list(failure_factors),
                "avg_quality_high": sum(r["quality_score"] for r in high_quality) / len(high_quality),
                "avg_quality_low": sum(r["quality_score"] for r in low_quality) / len(low_quality)
            }
            
            # Generate improvement suggestions
            self._generate_improvements(prompt_id)
    
    def _extract_prompt_patterns(self, prompts: list) -> list:
        """Extract common patterns from prompt text."""
        patterns = []
        
        # Common phrase patterns
        common_phrases = [
            "step-by-step", "detailed analysis", "specific examples",
            "code samples", "best practices", "security considerations",
            "performance optimization", "error handling"
        ]
        
        for phrase in common_phrases:
            phrase_count = sum(1 for prompt in prompts if phrase.lower() in prompt.lower())
            if phrase_count > len(prompts) * 0.6:  # 60% threshold
                patterns.append(f"includes_{phrase.replace(' ', '_')}")
        
        # Structural patterns
        if sum(1 for p in prompts if "ROLE:" in p) > len(prompts) * 0.7:
            patterns.append("has_role_definition")
        
        if sum(1 for p in prompts if "OUTPUT:" in p) > len(prompts) * 0.7:
            patterns.append("has_output_specification")
        
        if sum(1 for p in prompts if "CONSTRAINTS:" in p) > len(prompts) * 0.5:
            patterns.append("has_explicit_constraints")
        
        return patterns
    
    def _generate_improvements(self, prompt_id: str):
        """Generate specific improvement suggestions."""
        patterns = self.feedback_patterns[prompt_id]
        suggestions = []
        
        for success_factor in patterns["success_factors"]:
            if "step_by_step" in success_factor:
                suggestions.append("Add explicit step-by-step instruction format")
            elif "examples" in success_factor:
                suggestions.append("Include more concrete examples in prompt")
            elif "role_definition" in success_factor:
                suggestions.append("Strengthen role specification with domain expertise")
            elif "output_specification" in success_factor:
                suggestions.append("Add detailed output format requirements")
        
        for failure_factor in patterns["failure_factors"]:
            if "vague_instructions" in failure_factor:
                suggestions.append("Replace vague language with specific requirements")
            elif "missing_context" in failure_factor:
                suggestions.append("Add more context about expected use case")
        
        self.improvement_suggestions[prompt_id] = {
            "suggestions": suggestions,
            "confidence": min(len(patterns["success_factors"]) * 0.2, 1.0),
            "potential_improvement": patterns["avg_quality_high"] - patterns["avg_quality_low"]
        }
    
    def get_improvement_recommendations(self, prompt_id: str) -> dict:
        """Get specific recommendations for prompt improvement."""
        if prompt_id not in self.improvement_suggestions:
            return {"error": "Insufficient data for recommendations"}
        
        return self.improvement_suggestions[prompt_id]
```

---

## ðŸŽ¯ Production Integration Patterns

### **Enterprise Prompt Management**

```python
class EnterprisePromptManager:
    """Production-ready prompt management system."""
    
    def __init__(self):
        self.prompt_registry = {}
        self.version_control = {}
        self.access_control = {}
        self.audit_log = []
    
    def register_prompt(self, prompt_id: str, prompt_content: str, 
                       version: str, creator: str, approval_level: str):
        """Register new prompt with version control."""
        if prompt_id not in self.prompt_registry:
            self.prompt_registry[prompt_id] = {}
            self.version_control[prompt_id] = []
        
        # Version control
        version_entry = {
            "version": version,
            "content": prompt_content,
            "creator": creator,
            "created_date": datetime.now(),
            "approval_level": approval_level,
            "status": "active" if approval_level in ["approved", "production"] else "pending"
        }
        
        self.version_control[prompt_id].append(version_entry)
        
        # Update active version if approved
        if approval_level in ["approved", "production"]:
            self.prompt_registry[prompt_id]["active"] = version_entry
        
        # Audit log
        self.audit_log.append({
            "action": "register_prompt",
            "prompt_id": prompt_id,
            "version": version,
            "creator": creator,
            "timestamp": datetime.now()
        })
    
    def get_production_prompt(self, prompt_id: str, user_role: str) -> dict:
        """Get production-approved prompt with access control."""
        # Check access control
        if not self._check_access(prompt_id, user_role):
            return {"error": "Access denied", "prompt_id": prompt_id}
        
        if prompt_id not in self.prompt_registry:
            return {"error": "Prompt not found", "prompt_id": prompt_id}
        
        active_prompt = self.prompt_registry[prompt_id].get("active")
        if not active_prompt:
            return {"error": "No approved version available", "prompt_id": prompt_id}
        
        # Log access
        self.audit_log.append({
            "action": "access_prompt",
            "prompt_id": prompt_id,
            "user_role": user_role,
            "version": active_prompt["version"],
            "timestamp": datetime.now()
        })
        
        return {
            "prompt_id": prompt_id,
            "content": active_prompt["content"],
            "version": active_prompt["version"],
            "approval_level": active_prompt["approval_level"]
        }
    
    def _check_access(self, prompt_id: str, user_role: str) -> bool:
        """Check if user role has access to prompt."""
        # Simple role-based access control
        role_permissions = {
            "admin": ["all"],
            "senior_engineer": ["development", "testing", "performance"],
            "engineer": ["development", "testing"],
            "analyst": ["analysis", "reporting"]
        }
        
        prompt_category = self._get_prompt_category(prompt_id)
        user_permissions = role_permissions.get(user_role, [])
        
        return "all" in user_permissions or prompt_category in user_permissions
    
    def _get_prompt_category(self, prompt_id: str) -> str:
        """Determine prompt category for access control."""
        # Extract category from prompt ID or metadata
        if "security" in prompt_id.lower():
            return "security"
        elif "performance" in prompt_id.lower():
            return "performance"
        elif "analysis" in prompt_id.lower():
            return "analysis"
        else:
            return "development"
```

---

## ðŸŽ¯ Key Takeaways

### **Advanced Pattern Mastery**

1. **Few-Shot Excellence**: Strategic example selection dramatically improves output quality and consistency
2. **Reasoning Chains**: Structured thinking processes enable complex problem-solving capabilities
3. **Dynamic Adaptation**: Context-aware prompt evolution optimizes for specific scenarios
4. **Feedback Learning**: Continuous improvement through systematic outcome analysis

### **Production Readiness**

- **Enterprise Management**: Version control, access control, and audit trails for production systems
- **Performance Optimization**: Intelligent caching and example selection for efficiency
- **Quality Assurance**: Systematic feedback loops and improvement recommendations
- **Scalable Architecture**: Modular design patterns for maintainable prompt systems

---

## ðŸ”— Related Topics

**Prerequisites**:

- **Part A**: Structural foundations and role-based prompting
- **Part B**: Performance optimization and caching strategies

**Builds Upon**:

- [LLM Fundamentals](01_LLM-Fundamentals.md) - Understanding model capabilities
- [LLM Explainability](17_LLM-Explainability-Fundamentals.md) - Understanding model reasoning

**Enables**:

- Production AI agent development
- Enterprise AI system integration
- Advanced reasoning system design
- Automated prompt optimization systems

**Cross-References**:

- [AI Agents](../../07_AI-Agents/) - Implementing reasoning in AI systems
- [Software Design Principles](../../01_Development/02_software-design-principles/) - Architectural patterns for prompt systems
- [DevOps Best Practices](../../04_DevOps/) - Production deployment and monitoring

---

## Series Summary

This 3-part series covered:

- **Part A**: Structural foundations and role-based prompt engineering
- **Part B**: Performance optimization, caching, and cost management  
- **Part C**: Advanced reasoning, few-shot learning, and production integration

Together, these patterns enable the design of sophisticated, production-ready AI systems that deliver consistent, high-quality results at enterprise scale.
