# 2025-08-13 — Capture-only Notes

- What I did (bullets):
- Snippets / commands (if any):
- Links / sources (optional):
- Insight (one takeaway):
- Next tiny step (tomorrow):

## Term Frequency (TF) and TF-IDF

**What it does:**  

- **TF:** Shows how often each word appears in a sentence, compared to the total number of words.
- **TF-IDF:** Adjusts these counts to highlight words that are special or unique to each sentence, and downplays words that appear everywhere.

**Why it's useful:**  
TF-IDF helps computers find the most important words in your text, which is great for searching, sorting, or understanding meaning.

### How to read TF and TF‑IDF

- Term Frequency (TF): Within one sentence, count each word and divide by total words in that sentence. TF values are between 0 and 1.
- Inverse Document Frequency (IDF): Words that appear in many sentences get lower weight; rare words get higher weight.
- TF‑IDF = TF × IDF: High when a word is frequent in a sentence but rare across other sentences.

Tiny example
Sentences:

1) "this movie is awesome awesome"
2) "i do not say is good, but neither awesome"
3) "awesome? only a fool can say that"

Intuition:

- The word "awesome" appears in all three sentences, so its IDF weight is lower than a word that appears in only one sentence.
- A word that appears many times in a single sentence (like "awesome" in sentence 1) can still get a decent score, but very common words across sentences (like "is") get pushed down.

Reading the printed matrix:

- Rows are sentences; columns are words in the vocabulary.
- Larger numbers indicate words that are more important to that sentence.
- Compare columns: a word with small values in many rows is probably not very informative; a word with a large value in a single row is likely distinctive there.

#### Tiny numeric walk‑through

Let’s compute TF and IDF for the word "awesome" across the three sentences above.

1) Token counts

- Sentence 1: this(1), movie(1), is(1), awesome(2) → total words = 5
- Sentence 2: i(1), do(1), not(1), say(1), is(1), good(1), but(1), neither(1), awesome(1) → total words = 9
- Sentence 3: a(1), awesome(1), can(1), fool(1), only(1), say(1), that(1) → total words = 7

2) TF (per sentence)

- TF1(awesome) = 2 / 5 = 0.40
- TF2(awesome) = 1 / 9 ≈ 0.11
- TF3(awesome) = 1 / 7 ≈ 0.14

3) IDF (with smoothing like in code): idf = log((1 + N) / (1 + df)) + 1

- N = 3 sentences; df(awesome) = 3 (appears in all three)
- IDF(awesome) = log((1+3)/(1+3)) + 1 = log(1) + 1 = 1.0

4) TF‑IDF = TF × IDF

- TF‑IDF1(awesome) = 0.40 × 1.0 = 0.40
- TF‑IDF2(awesome) ≈ 0.11 × 1.0 ≈ 0.11
- TF‑IDF3(awesome) ≈ 0.14 × 1.0 ≈ 0.14

Interpretation

- "awesome" is still most important to sentence 1 due to repetition.
- Words rare across sentences (df close to 1) would get higher IDF and stand out in their sentence; words present everywhere get down‑weighted.

#### Tiny TF table (normalized counts)

Using the same three sentences:

1) "this movie is awesome awesome" (5 words)
2) "i do not say is good, but neither awesome" (9 words)
3) "awesome? only a fool can say that" (7 words)

Vocabulary (sorted): ["a", "awesome", "but", "can", "do", "fool", "good", "i", "is", "movie", "neither", "not", "only", "say", "that", "this"]

TF matrix (rows = sentences, columns = vocabulary; values rounded to 2 decimals):

| sentence | a | awesome | but | can | do | fool | good | i | is | movie | neither | not | only | say | that | this |
|---------:|:-:|:------:|:---:|:---:|:--:|:----:|:----:|:-:|:--:|:-----:|:-------:|:---:|:----:|:---:|:----:|:----:|
| 1        |0.00|  0.40  |0.00 |0.00 |0.00| 0.00 |0.00  |0.00|0.20| 0.20 | 0.00    |0.00 |0.00 |0.00 |0.00 |0.20 |
| 2        |0.00|  0.11  |0.11 |0.00 |0.11| 0.00 |0.11  |0.11|0.11| 0.00 | 0.11    |0.11 |0.00 |0.11 |0.00 |0.00 |
| 3        |0.14|  0.14  |0.00 |0.14 |0.00| 0.14 |0.00  |0.00|0.00| 0.00 | 0.00    |0.00 |0.14 |0.14 |0.14 |0.00 |

Notes

- Each row sums to 1.0 (up to rounding), because TF divides counts by total words in that sentence.
- TF emphasizes repetition within a sentence; IDF will reduce the weight of words spread across many sentences.

## Code for reference

```python
def compute_tf(sentences):
    """Compute the term frequency matrix for a list of sentences."""
    tokenized = [clean_tokenize(s) for s in sentences]
    vocabulary = sorted(set(w for sent in tokenized for w in sent))
    word_index = {word: i for i, word in enumerate(vocabulary)}
    tf = np.zeros((len(sentences), len(vocabulary)), dtype=np.float32)
    for i, words in enumerate(tokenized):
        word_count = len(words) if len(words) > 0 else 1
        for word in words:
            tf[i, word_index[word]] += 1 / word_count
    return tf, vocabulary


def compute_idf(sentences, vocabulary):
    """Compute the inverse document frequency with sklearn-style smoothing."""
    tokenized = [set(clean_tokenize(s)) for s in sentences]
    num_documents = len(sentences)
    idf = np.zeros(len(vocabulary), dtype=np.float32)
    word_index = {word: i for i, word in enumerate(vocabulary)}
    for word in vocabulary:
        df = sum(1 for sent in tokenized if word in sent)
        # sklearn-style: log((1 + N) / (1 + df)) + 1 → equals 1.0 for df == N
        idf[word_index[word]] = np.log((1 + num_documents) / (1 + df)) + 1.0
    return idf


def tf_idf(sentences):
    """Generate a TF-IDF matrix for a list of sentences."""
    tf, vocabulary = compute_tf(sentences)
    idf = compute_idf(sentences, vocabulary)
    tf_idf_matrix = tf * idf
    return vocabulary, tf_idf_matrix


vocabulary, tf_idf_matrix = tf_idf(corpus)
print("Vocabulary ({}):".format(len(vocabulary)), vocabulary[:20])
print("TF-IDF Matrix shape:", tf_idf_matrix.shape)
print(tf_idf_matrix)

# Optional: view TF / IDF / TF-IDF as tables

import pandas as pd
from IPython.display import display

tf_matrix, tf_vocab = compute_tf(corpus)
idf_vector = compute_idf(corpus, tf_vocab)


tf_df = pd.DataFrame(tf_matrix, columns=tf_vocab)
idf_df = pd.DataFrame([idf_vector], columns=tf_vocab)
tfidf_df = pd.DataFrame(tf_idf_matrix, columns=vocabulary)

print("TF table:")
display(tf_df.round(2))
print("IDF vector:")
display(idf_df.round(2))
print("TF-IDF table:")
display(tfidf_df.round(2))

# Show top TF-IDF terms per sentence
import numpy as np


def top_tfidf_per_doc(vocab, tfidf_matrix, top_k=3):
    for i in range(tfidf_matrix.shape[0]):
        row = tfidf_matrix[i]
        idxs = np.argsort(-row)[:top_k]
        pairs = [(vocab[j], float(row[j])) for j in idxs if row[j] > 0]
        print(f"Sentence {i+1}: {pairs}")


top_tfidf_per_doc(vocabulary, tf_idf_matrix, top_k=3)
```

## Repo: <https://github.com/Swamy-s-Tech-Skills-Academy-AI-ML-Data/llm-agents-learning>
