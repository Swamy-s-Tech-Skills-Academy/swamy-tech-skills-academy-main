# 2025-08-11 — Capture-only Notes

- What I did (bullets):
  - Wrote a plain-English explanation of one-hot encoding (tokens → rows, vocabulary → columns)
  - Clarified that each row has a single 1 indicating the token at that position
  - Created an example matrix for “I like pizza” and listed tokens for a longer sentence
  - Noted that one-hot encodes presence only (no meaning/similarity)
- Snippets / commands (if any):
  - Python: `print("Tokens:", tokens)` to inspect tokenization
- Links / sources (optional):
  - Reference page: [01_One-Hot-Encoding](../../03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/01_One-Hot-Encoding.md)
  - KB artifact: [2025-08-11_One-Hot-Encoding](../../01_LeadArchitectKnowledgeBase/AI-NLP/2025-08-11_One-Hot-Encoding/README.md)
  - Canonical code (external): [llm-agents-learning](https://github.com/Swamy-s-Tech-Skills-Academy-AI-ML-Data/llm-agents-learning)
  - Sources used today:
    - AI assistant: GitHub Copilot (prompted summaries/clarifications)
  - Note: Content captured here is original and synthesized per STSA guidelines (no verbatim copying)
- Insight (one takeaway):
  - One-hot encoding is simple and positional but loses semantics and scales with vocabulary size
- Next tiny step (today):
  - Implement a tiny `one_hot(tokens)` function and print the matrix for a short sentence

## Migration

- Concept captured in Reference Library: [01_One-Hot-Encoding](../../03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/01_One-Hot-Encoding.md)
- Evidence in Knowledge Base: [2025-08-11_One-Hot-Encoding](../../01_LeadArchitectKnowledgeBase/AI-NLP/2025-08-11_One-Hot-Encoding/)
- Canonical code: [llm-agents-learning](https://github.com/Swamy-s-Tech-Skills-Academy-AI-ML-Data/llm-agents-learning)
