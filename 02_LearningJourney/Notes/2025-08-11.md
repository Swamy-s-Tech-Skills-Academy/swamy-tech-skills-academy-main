# 2025-08-11 — Capture-only Notes

- What I did (bullets):
  - Wrote a plain-English explanation of one-hot encoding (tokens → rows, vocabulary → columns)
  - Clarified that each row has a single 1 indicating the token at that position
  - Created an example matrix for “I like pizza” and listed tokens for a longer sentence
  - Noted that one-hot encodes presence only (no meaning/similarity)
- Snippets / commands (if any):
  - Python: `print("Tokens:", tokens)` to inspect tokenization
- Links / sources (optional):
  - Reference page: [01_One-Hot-Encoding](/03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/01_One-Hot-Encoding.md)
  - KB artifact: [2025-08-11_One-Hot-Encoding](/01_LeadArchitectKnowledgeBase/AI-NLP/2025-08-11_One-Hot-Encoding/README.md)
  - Local example (canonical): `03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/examples/one_hot.py`
  - Sources used today:
    - AI assistant: GitHub Copilot (prompted summaries/clarifications)
  - Note: Content captured here is original and synthesized per STSA guidelines (no verbatim copying)
- Insight (one takeaway):
  - One-hot encoding is simple and positional but loses semantics and scales with vocabulary size
- Next tiny step (today):
  - Implement a tiny `one_hot(tokens)` function and print the matrix for a short sentence

## Migration

- Concept captured in Reference Library: [01_One-Hot-Encoding](/03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/01_One-Hot-Encoding.md)
- Evidence in Knowledge Base: [2025-08-11_One-Hot-Encoding](/01_LeadArchitectKnowledgeBase/AI-NLP/2025-08-11_One-Hot-Encoding/)
- Canonical code: `03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/examples/one_hot.py`
