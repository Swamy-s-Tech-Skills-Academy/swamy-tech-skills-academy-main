# 2025-08-11 — Capture-only Notes

- What I did (bullets):
  - Wrote a plain-English explanation of one-hot encoding (tokens → rows, vocabulary → columns)
  - Clarified that each row has a single 1 indicating the token at that position
  - Created an example matrix for “I like pizza” and listed tokens for a longer sentence
  - Noted that one-hot encodes presence only (no meaning/similarity)
- Snippets / commands (if any):
  - Python: `print("Tokens:", tokens)` to inspect tokenization
- Links / sources (optional):
  - Reference page: `../03_ReferenceLibrary/02_AI-and-ML/04_NaturalLanguageProcessing/01_Basics/01_One-Hot-Encoding.md`
  - KB artifact: `../01_LeadArchitectKnowledgeBase/AI-NLP/2025-08-11_One-Hot-Encoding/`
  - External repo: [llm-agents-learning](https://github.com/Swamy-s-Tech-Skills-Academy-AI-ML-Data/llm-agents-learning)
  - Self-authored explanation; no external sources used
- Insight (one takeaway):
  - One-hot encoding is simple and positional but loses semantics and scales with vocabulary size
- Next tiny step (today):
  - Implement a tiny `one_hot(tokens)` function and print the matrix for a short sentence

## One-Hot Encoding

**What it does:**  
Turns each word in a sentence into a simple list of numbers, where only one number is "on" (1) and the rest are "off" (0). This helps computers understand which words are present, without any math or meaning.

**Why it's useful:**  
It's a basic way to show computers what words are in a sentence, so they can start to "see" text.

**Example:**

Sentence: "I like pizza"

Vocabulary: ["i", "like", "pizza"]

One-hot encoding matrix:

| Word   | i | like | pizza |
|--------|---|------|-------|
| i      | 1 | 0    | 0     |
| like   | 0 | 1    | 0     |
| pizza  | 0 | 0    | 1     |

So, "i" becomes [1, 0, 0], "like" becomes [0, 1, 0], and "pizza" becomes [0, 0, 1].

This is how computers turn words into numbers using one-hot encoding!

### How Rows and Tokens Work in One-Hot Encoding

- Each **row** in the one-hot encoding matrix stands for a word (token) from your sentence, in the order they appear.
- For example, if your sentence is:
  `Should we go to a pizzeria or do you a prefer a restaurant?`
- The tokens will be:
  `['should', 'we', 'go', 'to', 'a', 'pizzeria', 'or', 'do', 'you', 'a', 'prefer', 'a', 'restaurant']`
- The matrix will have 13 rows (one for each token above).
- Each row shows which word from the vocabulary is present at that position in the sentence.

**Summary:**

- Number of rows = number of tokens (words) in your sentence.
- Number of columns = number of unique words (vocabulary) in your sentence.

### What are tokens?

- **Tokens** are just the individual words in your sentence, after cleaning (removing punctuation, making lowercase, etc.).
- For example, if your sentence is "Should we go to a pizzeria or do you a prefer a restaurant?", the tokens will be:
  `['should', 'we', 'go', 'to', 'a', 'pizzeria', 'or', 'do', 'you', 'a', 'prefer', 'a', 'restaurant']`
- You can see the tokens by adding this line to your code:

```python
print("Tokens:", tokens)
```

- This helps you understand how the sentence is split into words before any encoding happens.

### Understanding the One-Hot Encoding Output

- The output matrix has rows for each word in your sentence (in order) and columns for each unique word in the sentence (the vocabulary).
- Each row has a single `1` in the column that matches the word at that position; all other entries are `0`.
- This lets the computer know which word is present at each spot, but doesn't tell it anything about meaning or similarity yet.

### Vocabulary = Unique Words

- The vocabulary is simply the list of all unique words found in your sentence.
- For example, if your sentence is "Should we go to a pizzeria or do you a prefer a restaurant?", the vocabulary will be all the different words in that sentence, with no repeats.

**Summary:**

- One-hot encoding is a way for computers to turn words into numbers, so they can start working with text.
