# NLP Fundamentals - Basic Concepts

**Learning Track**: Natural Language Processing  
**Learning Level**: Beginner  
**Domain**: Text Processing Foundations

## 📚 Learning Progression

This directory contains foundational NLP concepts that every practitioner should understand before diving into advanced topics like transformers and LLMs.

### 🎯 Recommended Learning Path

```text
Text Representation Journey:
01_One-Hot-Encoding.md
    ↓ (understand basic text→numbers)
02_Tokenization-Basics.md  
    ↓ (learn text splitting fundamentals)
03_Bag-of-Words.md
    ↓ (simple counting approaches)
04_TF-IDF.md
    ↓ (weighted text representation)
05_Model-Tokenizer-Compatibility.md ← NEW!
    ↓ (critical for modern LLMs)
Advanced Text Representations →
```

### 📋 Topic Overview

#### **01_One-Hot-Encoding**

- **What**: Basic binary text representation
- **Why Important**: Foundation for understanding how computers process text
- **Learning Level**: Beginner
- **Time**: 15 minutes

#### **02_Tokenization-Basics**

- **What**: Breaking text into processable units
- **Why Important**: Essential preprocessing step for all NLP
- **Learning Level**: Beginner  
- **Time**: 20 minutes

#### **03_Bag-of-Words**

- **What**: Simple word counting representation
- **Why Important**: Introduces concept of text as numerical features
- **Learning Level**: Beginner
- **Time**: 25 minutes

#### **04_TF-IDF**

- **What**: Weighted text representation considering word importance
- **Why Important**: Bridge between simple counting and modern embeddings
- **Learning Level**: Beginner to Intermediate
- **Time**: 30 minutes

#### **05_Model-Tokenizer-Compatibility** ⭐ **Essential for LLM Work**

- **What**: Understanding why different models need specific tokenizers
- **Why Important**: Prevents critical errors when working with modern LLMs
- **Learning Level**: Beginner to Intermediate
- **Time**: 20 minutes

## 🔗 Connections to Advanced Topics

### Enables Understanding Of

- [Tokenization and Token IDs](../03_Tokenization-and-Token-IDs.md)
- [Word Embeddings](../02_Text-Representation/04_Word2Vec-and-Embeddings.md)
- [LLM Fundamentals](../../05_LargeLanguageModels/01_LLM-Fundamentals.md)
- [Transformer Architecture](../../03_DeepLearning/01_Transformer-Architecture.md)

### Prerequisites From Other Tracks

- Basic Python programming
- Understanding of vectors and matrices (linear algebra basics)
- Familiarity with machine learning concepts

## 💡 Key Learning Outcomes

After completing this basics section, you will:

- ✅ Understand how text is converted to numerical representations
- ✅ Know when to use different text representation methods
- ✅ Avoid critical tokenizer-model compatibility errors
- ✅ Be prepared for advanced NLP and LLM topics
- ✅ Understand the evolution from simple counting to modern embeddings

## 🎯 Next Steps

Once you've mastered these basics:

1. **Advanced Text Representations**: Move to `../02_Text-Representation/`
2. **Practical Applications**: Explore `../05_Text-Classification.md`
3. **Modern Approaches**: Study LLM and Transformer topics
4. **Hands-On Practice**: Try the examples in the `examples/` directory

---

**Last Updated**: September 2025  
**Review Schedule**: Quarterly with NLP track updates
