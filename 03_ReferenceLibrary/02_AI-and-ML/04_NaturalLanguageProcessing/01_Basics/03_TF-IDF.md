# 03_TF-IDF

Learning Level: Beginner  
Prerequisites: Tokenization, Bag-of-Words  
Estimated Time: 15â€“20 minutes

## ðŸŽ¯ Learning Objectives

- Explain TF, IDF, and TFÃ—IDF at a high level
- Interpret a small TF-IDF matrix
- Compute a tiny TF-IDF by hand for intuition

## Conceptual Foundation

- TF (term frequency): within a document, counts normalized by document length
- IDF (inverse document frequency): down-weights terms common across many documents
- TF-IDF = TF Ã— IDF: highlights terms that are frequent in a document but rare overall

## Code policy (single source of truth)

- Runnable code lives in the external canonical repo. This page shows reference-only pseudocode to explain the idea.
- If you need runnable examples, use the repo linked below. If no repo exists for this topic, ask for code and weâ€™ll generate it externally.

## Tiny Example (3 sentences)

1) "this movie is awesome awesome"  
2) "i do not say is good, but neither awesome"  
3) "awesome only a fool can say that"

Intuition:

- "awesome" appears in all 3 â†’ IDF lower than a rare word
- A repeated word in one sentence can still rank high if itâ€™s not ubiquitous

## Minimal Computation Sketch

- Compute TF per sentence: count(word)/len(sentence)
- Compute IDF with smoothing (sklearn-style): log((1+N)/(1+df)) + 1
- Multiply TF by IDF per term to get TF-IDF matrix

## Reference pseudocode (non-runnable)

```text
function compute_tf(sentences):
    tokenized := [clean_tokenize(s) for s in sentences]
    vocab := sorted(unique(all tokens))
    word_index := { word -> index }
    TF := zeros(num_sentences, vocab_size)
    for each sentence i, words in tokenized:
        denom := max(1, len(words))
        for each word w in words:
            TF[i, word_index[w]] += 1/denom
    return TF, vocab

function compute_idf(sentences, vocab):
    token_sets := [set(clean_tokenize(s)) for s in sentences]
    N := number of sentences
    word_index := { word -> index }
    IDF := zeros(vocab_size)
    for each word w in vocab:
        df := count of token_sets that contain w
        IDF[word_index[w]] := log((1+N)/(1+df)) + 1
    return IDF

function tf_idf(sentences):
    TF, vocab := compute_tf(sentences)
    IDF := compute_idf(sentences, vocab)
    return vocab, TF * IDF
```

## Canonical Code Location

- External repo (single source of truth for runnable code):
  - GitHub: [Swamy-s-Tech-Skills-Academy-AI-ML-Data/llm-agents-learning](https://github.com/Swamy-s-Tech-Skills-Academy-AI-ML-Data/llm-agents-learning)

## ðŸ”— Related Topics

- Prerequisites: `00_Tokenization.md`
- Related: `02_Bag-of-Words.md`
- Next: distributed embeddings (word2vec, GloVe)
