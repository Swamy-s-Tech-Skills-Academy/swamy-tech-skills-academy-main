{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cef8422",
   "metadata": {},
   "source": [
    "# 🤖 Transformers & LLMs: Interactive Learning Tutorial\n",
    "\n",
    "Welcome to the comprehensive hands-on guide to **Transformer Architecture**, **Large Language Models**, and **Prompt Engineering**!\n",
    "\n",
    "## 📚 What You'll Learn\n",
    "\n",
    "1. **Transformer Architecture** - Understanding the revolutionary architecture\n",
    "2. **How LLMs like GPT use Transformers** - From architecture to applications\n",
    "3. **LLM Training Process** - Pre-training and fine-tuning explained\n",
    "4. **Prompt Engineering** - Mastering the art of AI communication\n",
    "\n",
    "## 🛠️ Tools & Technologies\n",
    "\n",
    "- **Python** - Core programming language\n",
    "- **PyTorch** - Deep learning framework\n",
    "- **Transformers Library (Hugging Face)** - Pre-trained models and tools\n",
    "- **Matplotlib/Seaborn** - Data visualization\n",
    "- **Jupyter Notebook** - Interactive development environment\n",
    "\n",
    "> **💡 Architect's Note**: This notebook provides both theoretical understanding and practical implementation. Each section builds upon the previous one, creating a complete learning journey from basic concepts to advanced applications.\n",
    "\n",
    "---\n",
    "\n",
    "**Let's start building your expertise in modern AI architecture!** 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94a18b",
   "metadata": {},
   "source": [
    "## 1. 📦 Import Required Libraries\n",
    "\n",
    "First, let's import all the essential libraries we'll need for our Transformer and LLM exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93407c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
    "    GPT2LMHeadModel, GPT2Tokenizer,\n",
    "    BertModel, BertTokenizer,\n",
    "    pipeline, set_seed\n",
    ")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch for deep learning\n",
    "\n",
    "# Hugging Face Transformers for pre-trained models\n",
    "\n",
    "# Additional utilities\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16aa69",
   "metadata": {},
   "source": [
    "## 2. 🏗️ Visualizing Transformer Architecture\n",
    "\n",
    "Let's understand what makes Transformers so powerful by visualizing their key components.\n",
    "\n",
    "### What is a Transformer?\n",
    "\n",
    "A **Transformer** is a neural network architecture that revolutionized natural language processing by:\n",
    "\n",
    "- Processing all words in a sentence **simultaneously** (not one by one)\n",
    "- Using **attention mechanisms** to understand relationships between words\n",
    "- Enabling **parallel computation** for faster training and inference\n",
    "\n",
    "### Why Transformers Over RNNs/LSTMs?\n",
    "\n",
    "Before Transformers, models like RNNs and LSTMs were:\n",
    "\n",
    "- ❌ **Sequential** - processed words one by one, step by step\n",
    "- ❌ **Slow** - couldn't parallelize effectively on modern GPUs\n",
    "- ❌ **Limited memory** - struggled with long-range dependencies\n",
    "- ❌ **Bottleneck** - information had to flow through a single hidden state\n",
    "\n",
    "Transformers changed this by:\n",
    "\n",
    "- ✅ **Parallel processing** - all words processed simultaneously\n",
    "- ✅ **Global attention** - can relate any word to any other word directly\n",
    "- ✅ **Faster training** - much more efficient on modern hardware\n",
    "- ✅ **Better long-range understanding** - no information bottleneck\n",
    "\n",
    "### The Transformer Revolution\n",
    "\n",
    "The key insight: **\"Attention is All You Need\"** (Vaswani et al., 2017)\n",
    "\n",
    "Instead of relying on recurrence or convolution, Transformers use **self-attention** to:\n",
    "\n",
    "- Let each word \"look at\" every other word in the sentence\n",
    "- Learn which words are most relevant for understanding each position\n",
    "- Build rich, contextual representations through multiple attention layers\n",
    "\n",
    "This breakthrough enabled the creation of all modern LLMs including:\n",
    "\n",
    "- **GPT series** (Decoder-only Transformers)\n",
    "- **BERT** (Encoder-only Transformers)\n",
    "- **T5/BART** (Encoder-Decoder Transformers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d7d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual representation of Transformer Architecture\n",
    "def visualize_transformer_architecture():\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 8))\n",
    "\n",
    "    # Transformer Components\n",
    "    components = {\n",
    "        'Input Embeddings': {'color': '#FFE5B4', 'position': 0},\n",
    "        'Positional Encoding': {'color': '#FFCCCB', 'position': 1},\n",
    "        'Multi-Head Attention': {'color': '#ADD8E6', 'position': 2},\n",
    "        'Feed Forward': {'color': '#90EE90', 'position': 3},\n",
    "        'Layer Normalization': {'color': '#DDA0DD', 'position': 4},\n",
    "        'Output Layer': {'color': '#F0E68C', 'position': 5}\n",
    "    }\n",
    "\n",
    "    # Plot 1: Transformer Block Components\n",
    "    ax1.set_title('🔧 Transformer Block Components',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    y_positions = list(range(len(components)))\n",
    "    colors = [comp['color'] for comp in components.values()]\n",
    "    component_names = list(components.keys())\n",
    "\n",
    "    bars = ax1.barh(y_positions, [1]*len(components), color=colors, alpha=0.7)\n",
    "    ax1.set_yticks(y_positions)\n",
    "    ax1.set_yticklabels(component_names)\n",
    "    ax1.set_xlabel('Component Flow')\n",
    "    ax1.set_xlim(0, 1.2)\n",
    "\n",
    "    # Add annotations\n",
    "    for i, (name, _) in enumerate(components.items()):\n",
    "        ax1.text(0.5, i, name, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "    # Plot 2: Attention Mechanism Visualization\n",
    "    ax2.set_title('👁️ Self-Attention Mechanism',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Create attention matrix visualization\n",
    "    sentence = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "    attention_matrix = np.random.rand(5, 5)\n",
    "    # Make it more realistic (words attend more to themselves and neighbors)\n",
    "    for i in range(5):\n",
    "        attention_matrix[i, i] = 0.8  # Self-attention\n",
    "        if i > 0:\n",
    "            attention_matrix[i, i-1] = 0.6  # Previous word\n",
    "        if i < 4:\n",
    "            attention_matrix[i, i+1] = 0.6  # Next word\n",
    "\n",
    "    im = ax2.imshow(attention_matrix, cmap='Blues', aspect='auto')\n",
    "    ax2.set_xticks(range(len(sentence)))\n",
    "    ax2.set_yticks(range(len(sentence)))\n",
    "    ax2.set_xticklabels(sentence)\n",
    "    ax2.set_yticklabels(sentence)\n",
    "    ax2.set_xlabel('Keys')\n",
    "    ax2.set_ylabel('Queries')\n",
    "\n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax2, shrink=0.8, label='Attention Weight')\n",
    "\n",
    "    # Plot 3: Encoder vs Decoder\n",
    "    ax3.set_title('🔄 Encoder vs Decoder', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Encoder stack\n",
    "    encoder_layers = ['Input\\nEmbedding',\n",
    "                      'Multi-Head\\nAttention', 'Feed\\nForward', 'Output']\n",
    "    decoder_layers = ['Output\\nEmbedding', 'Masked\\nAttention',\n",
    "                      'Cross\\nAttention', 'Feed\\nForward', 'Linear\\n& Softmax']\n",
    "\n",
    "    # Plot encoder\n",
    "    for i, layer in enumerate(encoder_layers):\n",
    "        ax3.add_patch(plt.Rectangle((0, i*0.8), 0.4, 0.6,\n",
    "                                    facecolor='lightblue', edgecolor='black', alpha=0.7))\n",
    "        ax3.text(0.2, i*0.8 + 0.3, layer, ha='center',\n",
    "                 va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    # Plot decoder\n",
    "    for i, layer in enumerate(decoder_layers):\n",
    "        ax3.add_patch(plt.Rectangle((0.6, i*0.8), 0.4, 0.6,\n",
    "                                    facecolor='lightgreen', edgecolor='black', alpha=0.7))\n",
    "        ax3.text(0.8, i*0.8 + 0.3, layer, ha='center',\n",
    "                 va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "    ax3.set_xlim(-0.1, 1.1)\n",
    "    ax3.set_ylim(-0.1, 4)\n",
    "    ax3.set_xticks([0.2, 0.8])\n",
    "    ax3.set_xticklabels(['Encoder', 'Decoder'], fontweight='bold')\n",
    "    ax3.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create the visualization\n",
    "visualize_transformer_architecture()\n",
    "\n",
    "print(\"📊 Transformer Architecture Visualized!\")\n",
    "print(\"\\n💡 Key Insights:\")\n",
    "print(\"1. Transformers process all words simultaneously using attention\")\n",
    "print(\"2. Self-attention helps words 'look at' each other to understand context\")\n",
    "print(\"3. Encoders understand input, Decoders generate output\")\n",
    "print(\"4. Multiple layers stack to create deep understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce1a57",
   "metadata": {},
   "source": [
    "## 3. 🧠 Implementing Self-Attention Mechanism\n",
    "\n",
    "Now let's implement the heart of the Transformer: **Self-Attention**. This mechanism allows each word to \"attend\" to all other words in the sequence.\n",
    "\n",
    "### How Self-Attention Works\n",
    "\n",
    "The self-attention mechanism computes three vectors for each word:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\" - What information does this word need?\n",
    "- **Key (K)**: \"What do I represent?\" - What information does this word provide?\n",
    "- **Value (V)**: \"What information do I contain?\" - The actual information to retrieve\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "The famous attention formula that powers all modern LLMs:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q * K^T / √d_k) * V\n",
    "```\n",
    "\n",
    "**Step by step:**\n",
    "\n",
    "1. **Compute similarities**: Q \\* K^T gives attention scores (how much each word should attend to every other word)\n",
    "2. **Scale**: Divide by √d_k to prevent very large values that make softmax too \"sharp\"\n",
    "3. **Normalize**: Softmax ensures attention weights sum to 1\n",
    "4. **Aggregate**: Multiply by V to get the final attended representation\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "- **Parallelizable**: All positions computed simultaneously\n",
    "- **Dynamic**: Attention weights change based on context\n",
    "- **Global**: Each word can attend to any other word\n",
    "- **Learnable**: Q, K, V matrices are learned during training\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Instead of one attention operation, Transformers use **multiple attention \"heads\"**:\n",
    "\n",
    "- Each head learns different types of relationships\n",
    "- Some might focus on syntax, others on semantics\n",
    "- Results are concatenated and projected back\n",
    "- Allows the model to attend to different representation subspaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a227d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention mechanism implementation from scratch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (self.head_dim * heads ==\n",
    "                embed_size), \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        # Linear transformations for Q, K, V\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]  # Batch size\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split embedding into self.heads pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, head_dim)\n",
    "        # keys shape: (N, key_len, heads, head_dim)\n",
    "        # energy shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
    "\n",
    "        # Apply attention to values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out, attention\n",
    "\n",
    "# Demonstrate self-attention with a simple example\n",
    "\n",
    "\n",
    "def demonstrate_self_attention():\n",
    "    print(\"🔍 Demonstrating Self-Attention Mechanism\")\n",
    "\n",
    "    # Create sample input\n",
    "    batch_size = 1\n",
    "    seq_length = 5  # \"The cat sat on mat\"\n",
    "    embed_size = 256\n",
    "    heads = 8\n",
    "\n",
    "    # Sample input embeddings (normally these would come from word embeddings)\n",
    "    input_embeddings = torch.randn(batch_size, seq_length, embed_size)\n",
    "\n",
    "    # Create self-attention layer\n",
    "    attention_layer = SelfAttention(embed_size, heads)\n",
    "\n",
    "    # Forward pass\n",
    "    output, attention_weights = attention_layer(\n",
    "        input_embeddings, input_embeddings, input_embeddings, mask=None\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Input shape: {input_embeddings.shape}\")\n",
    "    print(f\"✅ Output shape: {output.shape}\")\n",
    "    print(f\"✅ Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "    # Visualize attention weights for the first head\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Extract attention weights for first head\n",
    "    first_head_attention = attention_weights[0, 0].detach().numpy()\n",
    "\n",
    "    # Create subplot for attention visualization\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.heatmap(first_head_attention, annot=True, fmt='.3f', cmap='Blues',\n",
    "                xticklabels=[f'Pos{i}' for i in range(seq_length)],\n",
    "                yticklabels=[f'Pos{i}' for i in range(seq_length)])\n",
    "    plt.title('🎯 Self-Attention Weights (Head 1)')\n",
    "    plt.xlabel('Key Positions')\n",
    "    plt.ylabel('Query Positions')\n",
    "\n",
    "    # Show attention scores distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    attention_mean = attention_weights.mean(\n",
    "        dim=1)[0].detach().numpy()  # Average across heads\n",
    "    plt.plot(attention_mean.flatten(), 'o-', color='blue', alpha=0.7)\n",
    "    plt.title('📊 Average Attention Distribution')\n",
    "    plt.xlabel('Position Pairs')\n",
    "    plt.ylabel('Attention Score')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Show embedding changes\n",
    "    plt.subplot(2, 2, 3)\n",
    "    input_norm = torch.norm(input_embeddings, dim=2)[0].detach().numpy()\n",
    "    output_norm = torch.norm(output, dim=2)[0].detach().numpy()\n",
    "\n",
    "    positions = list(range(seq_length))\n",
    "    plt.bar([p - 0.2 for p in positions], input_norm, width=0.4,\n",
    "            label='Input', alpha=0.7, color='lightcoral')\n",
    "    plt.bar([p + 0.2 for p in positions], output_norm, width=0.4,\n",
    "            label='Output', alpha=0.7, color='lightblue')\n",
    "    plt.title('📈 Embedding Magnitude Changes')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.xticks(positions, [f'Pos{i}' for i in positions])\n",
    "\n",
    "    # Show computational complexity\n",
    "    plt.subplot(2, 2, 4)\n",
    "    seq_lengths = [10, 50, 100, 200, 500, 1000]\n",
    "    complexity = [s**2 for s in seq_lengths]  # O(n²) complexity\n",
    "\n",
    "    plt.plot(seq_lengths, complexity, 'o-', color='red', linewidth=2)\n",
    "    plt.title('⚡ Self-Attention Complexity O(n²)')\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Operations (n²)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "output, attention_weights = demonstrate_self_attention()\n",
    "\n",
    "print(\"\\n💡 Key Self-Attention Insights:\")\n",
    "print(\"1. Each position can attend to all other positions\")\n",
    "print(\"2. Attention weights show which words are most relevant to each other\")\n",
    "print(\"3. The mechanism allows parallel computation across all positions\")\n",
    "print(\"4. Computational complexity is O(n²) with sequence length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5cef86",
   "metadata": {},
   "source": [
    "## 4. 🏗️ Building a Simple Transformer Block\n",
    "\n",
    "Now let's build a complete Transformer block by combining self-attention with other essential components:\n",
    "\n",
    "### Transformer Block Components:\n",
    "\n",
    "1. **Multi-Head Self-Attention** - What we just implemented\n",
    "2. **Position-wise Feed-Forward Network** - Processes each position independently\n",
    "3. **Residual Connections** - Helps with gradient flow\n",
    "4. **Layer Normalization** - Stabilizes training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5533ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer Block with all components\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Multi-head self-attention\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        # Self-attention with residual connection\n",
    "        attention_output, attention_weights = self.attention(\n",
    "            value, key, query, mask)\n",
    "\n",
    "        # Add & Norm (residual connection + layer norm)\n",
    "        x = self.dropout(self.norm1(attention_output + query))\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        forward_output = self.feed_forward(x)\n",
    "\n",
    "        # Add & Norm\n",
    "        out = self.dropout(self.norm2(forward_output + x))\n",
    "\n",
    "        return out, attention_weights\n",
    "\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Transformer model with multiple blocks\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, num_layers, heads,\n",
    "                 device, forward_expansion, dropout, max_length):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "\n",
    "        # Word embeddings\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(\n",
    "            N, seq_length).to(self.device)\n",
    "\n",
    "        # Combine word and position embeddings\n",
    "        out = self.dropout(\n",
    "            self.word_embedding(x) + self.position_embedding(positions)\n",
    "        )\n",
    "\n",
    "        attention_weights_all = []\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for layer in self.layers:\n",
    "            out, attention_weights = layer(out, out, out, mask)\n",
    "            attention_weights_all.append(attention_weights)\n",
    "\n",
    "        return out, attention_weights_all\n",
    "\n",
    "# Demonstrate the complete transformer\n",
    "\n",
    "\n",
    "def demonstrate_transformer_block():\n",
    "    print(\"🏗️ Building and Testing Complete Transformer\")\n",
    "\n",
    "    # Model hyperparameters\n",
    "    vocab_size = 1000\n",
    "    embed_size = 256\n",
    "    num_layers = 2\n",
    "    heads = 8\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    forward_expansion = 4\n",
    "    dropout = 0.1\n",
    "    max_length = 100\n",
    "\n",
    "    # Create model\n",
    "    model = SimpleTransformer(\n",
    "        vocab_size, embed_size, num_layers, heads,\n",
    "        device, forward_expansion, dropout, max_length\n",
    "    ).to(device)\n",
    "\n",
    "    # Sample input (token IDs)\n",
    "    batch_size = 2\n",
    "    seq_length = 10\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)\n",
    "\n",
    "    # No masking for this example\n",
    "    mask = None\n",
    "\n",
    "    print(\n",
    "        f\"📊 Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"📊 Input shape: {x.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output, all_attention_weights = model(x, mask)\n",
    "\n",
    "    print(f\"📊 Output shape: {output.shape}\")\n",
    "    print(\n",
    "        f\"📊 Number of attention weight tensors: {len(all_attention_weights)}\")\n",
    "\n",
    "    # Visualize model architecture\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot 1: Model parameter distribution\n",
    "    plt.subplot(2, 3, 1)\n",
    "    param_counts = []\n",
    "    layer_names = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            param_counts.append(param.numel())\n",
    "            layer_names.append(name.split('.')[0])\n",
    "\n",
    "    # Group by layer type\n",
    "    layer_counts = {}\n",
    "    for name, count in zip(layer_names, param_counts):\n",
    "        if name in layer_counts:\n",
    "            layer_counts[name] += count\n",
    "        else:\n",
    "            layer_counts[name] = count\n",
    "\n",
    "    plt.bar(range(len(layer_counts)), list(layer_counts.values()),\n",
    "            color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(layer_counts)])\n",
    "    plt.xticks(range(len(layer_counts)), list(\n",
    "        layer_counts.keys()), rotation=45)\n",
    "    plt.title('📊 Parameters by Layer Type')\n",
    "    plt.ylabel('Parameter Count')\n",
    "\n",
    "    # Plot 2: Attention patterns from first layer\n",
    "    plt.subplot(2, 3, 2)\n",
    "    # First sample, first head\n",
    "    first_layer_attention = all_attention_weights[0][0, 0].cpu().numpy()\n",
    "    sns.heatmap(first_layer_attention, cmap='Blues', square=True)\n",
    "    plt.title('🎯 Layer 1 Attention Pattern')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "\n",
    "    # Plot 3: Attention patterns from last layer\n",
    "    plt.subplot(2, 3, 3)\n",
    "    # First sample, first head\n",
    "    last_layer_attention = all_attention_weights[-1][0, 0].cpu().numpy()\n",
    "    sns.heatmap(last_layer_attention, cmap='Reds', square=True)\n",
    "    plt.title('🎯 Layer 2 Attention Pattern')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "\n",
    "    # Plot 4: Output embedding norms\n",
    "    plt.subplot(2, 3, 4)\n",
    "    output_norms = torch.norm(output[0], dim=1).cpu().numpy()  # First sample\n",
    "    plt.plot(output_norms, 'o-', color='purple', alpha=0.7)\n",
    "    plt.title('📈 Output Embedding Magnitudes')\n",
    "    plt.xlabel('Position')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: Attention head diversity\n",
    "    plt.subplot(2, 3, 5)\n",
    "    head_entropies = []\n",
    "    for head in range(heads):\n",
    "        attention_dist = all_attention_weights[0][0, head].cpu().numpy()\n",
    "        # Calculate entropy for each query position\n",
    "        entropies = []\n",
    "        for i in range(attention_dist.shape[0]):\n",
    "            prob_dist = attention_dist[i] + 1e-9  # Add small epsilon\n",
    "            entropy = -np.sum(prob_dist * np.log(prob_dist))\n",
    "            entropies.append(entropy)\n",
    "        head_entropies.append(np.mean(entropies))\n",
    "\n",
    "    plt.bar(range(heads), head_entropies, color='orange', alpha=0.7)\n",
    "    plt.title('🔍 Attention Head Diversity (Entropy)')\n",
    "    plt.xlabel('Head Number')\n",
    "    plt.ylabel('Average Entropy')\n",
    "    plt.xticks(range(heads))\n",
    "\n",
    "    # Plot 6: Model complexity comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    model_sizes = ['Small', 'Base', 'Large', 'XL']\n",
    "    param_counts = [12e6, 110e6, 340e6, 1.5e9]  # Approximate parameter counts\n",
    "\n",
    "    plt.bar(model_sizes, param_counts, color=[\n",
    "            'lightblue', 'orange', 'lightcoral', 'red'])\n",
    "    plt.title('🏗️ Transformer Model Sizes')\n",
    "    plt.ylabel('Parameters (Millions)')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Add annotations\n",
    "    for i, count in enumerate(param_counts):\n",
    "        plt.text(i, count, f'{count/1e6:.0f}M', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, output, all_attention_weights\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "model, output, attention_weights = demonstrate_transformer_block()\n",
    "\n",
    "print(\"\\n💡 Transformer Block Insights:\")\n",
    "print(\"1. Combines self-attention with feed-forward processing\")\n",
    "print(\"2. Residual connections help gradients flow through deep networks\")\n",
    "print(\"3. Layer normalization stabilizes training\")\n",
    "print(\"4. Multiple heads capture different types of relationships\")\n",
    "print(\"5. Positional encoding gives the model a sense of word order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7ed71",
   "metadata": {},
   "source": [
    "## 5. 🤗 Using Pretrained Transformers with Hugging Face\n",
    "\n",
    "Now let's see how to use real, pretrained Transformer models! Hugging Face provides an amazing library with thousands of pretrained models.\n",
    "\n",
    "### Popular Transformer Models:\n",
    "\n",
    "- **BERT**: Bidirectional Encoder (great for understanding)\n",
    "- **GPT**: Autoregressive Decoder (great for generation)\n",
    "- **T5**: Text-to-Text Transfer (great for various tasks)\n",
    "- **RoBERTa**: Robustly Optimized BERT\n",
    "\n",
    "Let's explore how to use these powerful models!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc856388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore different pretrained models\n",
    "def explore_pretrained_models():\n",
    "    print(\"🤗 Exploring Pretrained Transformer Models\")\n",
    "\n",
    "    # Model configurations\n",
    "    models_config = {\n",
    "        'BERT': {\n",
    "            'model_name': 'bert-base-uncased',\n",
    "            'use_case': 'Text Understanding & Classification',\n",
    "            'architecture': 'Encoder-only'\n",
    "        },\n",
    "        'GPT-2': {\n",
    "            'model_name': 'gpt2',\n",
    "            'use_case': 'Text Generation',\n",
    "            'architecture': 'Decoder-only'\n",
    "        },\n",
    "        'DistilBERT': {\n",
    "            'model_name': 'distilbert-base-uncased',\n",
    "            'use_case': 'Lightweight Text Understanding',\n",
    "            'architecture': 'Encoder-only (Distilled)'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\n📋 Available Models:\")\n",
    "    for name, config in models_config.items():\n",
    "        print(f\"  {name}: {config['use_case']} ({config['architecture']})\")\n",
    "\n",
    "    return models_config\n",
    "\n",
    "# Demonstrate BERT for text understanding\n",
    "\n",
    "\n",
    "def demonstrate_bert():\n",
    "    print(\"\\n🔍 BERT: Bidirectional Encoder Representations from Transformers\")\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Sample texts for analysis\n",
    "    texts = [\n",
    "        \"The transformer architecture revolutionized natural language processing.\",\n",
    "        \"Attention is all you need for understanding language.\",\n",
    "        \"BERT uses bidirectional context to understand words.\"\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n📊 BERT Model Info:\")\n",
    "    print(f\"  - Parameters: ~110M\")\n",
    "    print(f\"  - Architecture: 12 layers, 12 attention heads\")\n",
    "    print(f\"  - Vocabulary: {tokenizer.vocab_size:,} tokens\")\n",
    "\n",
    "    # Process texts with BERT\n",
    "    embeddings_data = []\n",
    "    attention_data = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"\\n📝 Processing: '{text}'\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\",\n",
    "                           padding=True, truncation=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "        print(f\"   Tokens: {tokens}\")\n",
    "\n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "        # Extract embeddings and attention\n",
    "        # [seq_len, hidden_size]\n",
    "        last_hidden_states = outputs.last_hidden_state[0]\n",
    "        attention_weights = outputs.attentions[-1][0]  # Last layer, first head\n",
    "\n",
    "        embeddings_data.append(last_hidden_states)\n",
    "        attention_data.append(attention_weights)\n",
    "\n",
    "        print(f\"   Embedding shape: {last_hidden_states.shape}\")\n",
    "\n",
    "    # Visualize BERT analysis\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot 1: Token embeddings similarity\n",
    "    plt.subplot(2, 3, 1)\n",
    "    # Compute similarity between first and second text\n",
    "    emb1 = embeddings_data[0].mean(dim=0)  # Average pooling\n",
    "    emb2 = embeddings_data[1].mean(dim=0)\n",
    "    emb3 = embeddings_data[2].mean(dim=0)\n",
    "\n",
    "    # Cosine similarity\n",
    "    sim_1_2 = F.cosine_similarity(emb1, emb2, dim=0).item()\n",
    "    sim_1_3 = F.cosine_similarity(emb1, emb3, dim=0).item()\n",
    "    sim_2_3 = F.cosine_similarity(emb2, emb3, dim=0).item()\n",
    "\n",
    "    similarities = [sim_1_2, sim_1_3, sim_2_3]\n",
    "    labels = ['Text 1-2', 'Text 1-3', 'Text 2-3']\n",
    "\n",
    "    bars = plt.bar(labels, similarities, color=[\n",
    "                   'skyblue', 'lightcoral', 'lightgreen'])\n",
    "    plt.title('📊 BERT: Text Similarity')\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "    # Add value annotations\n",
    "    for bar, sim in zip(bars, similarities):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                 f'{sim:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    # Plot 2: Attention pattern for first text\n",
    "    plt.subplot(2, 3, 2)\n",
    "    att_matrix = attention_data[0][0].numpy()  # First head\n",
    "    tokens_first = tokenizer.convert_ids_to_tokens(\n",
    "        tokenizer(texts[0], return_tensors=\"pt\")['input_ids'][0]\n",
    "    )\n",
    "\n",
    "    sns.heatmap(att_matrix,\n",
    "                xticklabels=tokens_first, yticklabels=tokens_first,\n",
    "                cmap='Blues', square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('🎯 BERT Attention Pattern')\n",
    "    plt.xlabel('Keys')\n",
    "    plt.ylabel('Queries')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    # Plot 3: Embedding dimensions analysis\n",
    "    plt.subplot(2, 3, 3)\n",
    "    emb_norms = [torch.norm(emb, dim=1).numpy() for emb in embeddings_data]\n",
    "\n",
    "    for i, norms in enumerate(emb_norms):\n",
    "        plt.plot(norms, 'o-', label=f'Text {i+1}', alpha=0.7)\n",
    "\n",
    "    plt.title('📈 Token Embedding Magnitudes')\n",
    "    plt.xlabel('Token Position')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Vocabulary coverage\n",
    "    plt.subplot(2, 3, 4)\n",
    "    vocab_stats = {\n",
    "        'Total Vocab': tokenizer.vocab_size,\n",
    "        'Special Tokens': len(tokenizer.all_special_tokens),\n",
    "        'Subword Units': tokenizer.vocab_size - len(tokenizer.all_special_tokens)\n",
    "    }\n",
    "\n",
    "    plt.pie(vocab_stats.values(), labels=vocab_stats.keys(), autopct='%1.1f%%',\n",
    "            colors=['lightblue', 'orange', 'lightgreen'])\n",
    "    plt.title('🔤 BERT Vocabulary Composition')\n",
    "\n",
    "    # Plot 5: Layer-wise attention head count\n",
    "    plt.subplot(2, 3, 5)\n",
    "    layers = list(range(1, 13))  # BERT has 12 layers\n",
    "    heads_per_layer = [12] * 12  # 12 heads per layer\n",
    "\n",
    "    plt.bar(layers, heads_per_layer, color='gold', alpha=0.7)\n",
    "    plt.title('🏗️ BERT Architecture: Heads per Layer')\n",
    "    plt.xlabel('Layer Number')\n",
    "    plt.ylabel('Number of Attention Heads')\n",
    "    plt.xticks(layers)\n",
    "\n",
    "    # Plot 6: Model size comparison\n",
    "    plt.subplot(2, 3, 6)\n",
    "    model_sizes = {\n",
    "        'BERT-base': 110,\n",
    "        'BERT-large': 340,\n",
    "        'DistilBERT': 66,\n",
    "        'RoBERTa-base': 125\n",
    "    }\n",
    "\n",
    "    plt.bar(model_sizes.keys(), model_sizes.values(),\n",
    "            color=['blue', 'darkblue', 'lightblue', 'green'])\n",
    "    plt.title('📊 BERT Family Model Sizes')\n",
    "    plt.ylabel('Parameters (Millions)')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return embeddings_data, attention_data\n",
    "\n",
    "\n",
    "# Run the demonstrations\n",
    "models_config = explore_pretrained_models()\n",
    "embeddings, attention = demonstrate_bert()\n",
    "\n",
    "print(\"\\n💡 Pretrained Model Benefits:\")\n",
    "print(\"1. No need to train from scratch - saves time and compute\")\n",
    "print(\"2. Models are trained on massive datasets with rich knowledge\")\n",
    "print(\"3. Can be fine-tuned for specific tasks\")\n",
    "print(\"4. Consistent APIs across different model architectures\")\n",
    "print(\"5. Community contributions make them continuously better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a400a7a",
   "metadata": {},
   "source": [
    "## 6. 🤖 Demonstrating GPT-Style Text Generation\n",
    "\n",
    "GPT (Generative Pre-trained Transformer) uses only the **decoder** part of the Transformer architecture. It works by:\n",
    "\n",
    "- Reading input text and **predicting the next word** one by one\n",
    "- Using attention to check what has already been written\n",
    "- Never looking at future words — only past words (causal attention)\n",
    "\n",
    "### Key Differences: GPT vs Full Transformer\n",
    "\n",
    "**GPT (Decoder-Only):**\n",
    "\n",
    "- ✅ **Autoregressive**: Generates text left-to-right, one token at a time\n",
    "- ✅ **Causal masking**: Can only see previous words, not future ones\n",
    "- ✅ **Text generation**: Optimized for creating new text\n",
    "- ✅ **Simpler architecture**: Single stack of decoder blocks\n",
    "\n",
    "**Full Transformer (Encoder-Decoder):**\n",
    "\n",
    "- 🔄 **Encoder**: Processes entire input simultaneously\n",
    "- 🔄 **Decoder**: Generates output while attending to encoder\n",
    "- 🔄 **Translation tasks**: Designed for input→output transformations\n",
    "\n",
    "### How GPT Generates Text\n",
    "\n",
    "1. **Start with prompt**: \"The future of AI is\"\n",
    "2. **Predict next word**: Model calculates probabilities for all possible next words\n",
    "3. **Sample/choose**: Select next word (e.g., \"bright\")\n",
    "4. **Update context**: \"The future of AI is bright\"\n",
    "5. **Repeat**: Continue until stopping condition\n",
    "\n",
    "### Generation Strategies\n",
    "\n",
    "- **Greedy**: Always pick the most probable word (deterministic but repetitive)\n",
    "- **Sampling**: Pick from probability distribution (more creative)\n",
    "- **Top-k**: Only consider k most likely words\n",
    "- **Temperature**: Control randomness (low = conservative, high = creative)\n",
    "\n",
    "Let's explore how GPT generates text step by step!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d675615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model for text generation\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"🔄 Loading GPT-2 model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"✅ GPT-2 model loaded successfully!\")\n",
    "\n",
    "\n",
    "def generate_text_step_by_step(prompt, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text step by step showing the process\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Starting prompt: '{prompt}'\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    generated_text = prompt\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length - len(input_ids[0])):\n",
    "            # Get model predictions\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get probabilities for next token\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # Sample next token\n",
    "            next_token_id = torch.multinomial(probs, 1)\n",
    "\n",
    "            # Decode and display\n",
    "            next_token = tokenizer.decode(\n",
    "                next_token_id, skip_special_tokens=True)\n",
    "            print(f\"Step {step + 1}: '{generated_text}' + '{next_token}'\")\n",
    "\n",
    "            # Update for next iteration\n",
    "            input_ids = torch.cat(\n",
    "                [input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
    "            generated_text += next_token\n",
    "\n",
    "            # Stop if we hit end token\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Demo: Step-by-step generation\n",
    "prompt = \"Artificial intelligence will\"\n",
    "result = generate_text_step_by_step(prompt, max_length=20)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Final generated text: '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d982c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different generation strategies\n",
    "def compare_generation_methods(prompt, max_length=30):\n",
    "    \"\"\"\n",
    "    Compare different text generation methods\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Prompt: '{prompt}'\\n\")\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Method 1: Greedy Decoding (always pick most likely)\n",
    "    with torch.no_grad():\n",
    "        greedy_output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    greedy_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Method 2: Sampling with temperature\n",
    "    with torch.no_grad():\n",
    "        sample_output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    sample_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Method 3: Top-k sampling\n",
    "    with torch.no_grad():\n",
    "        topk_output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    topk_text = tokenizer.decode(topk_output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"🔸 Greedy Decoding (deterministic):\")\n",
    "    print(f\"   '{greedy_text}'\\n\")\n",
    "\n",
    "    print(\"🔸 Temperature Sampling (creative):\")\n",
    "    print(f\"   '{sample_text}'\\n\")\n",
    "\n",
    "    print(\"🔸 Top-k Sampling (balanced):\")\n",
    "    print(f\"   '{topk_text}'\\n\")\n",
    "\n",
    "\n",
    "# Test different generation methods\n",
    "test_prompts = [\n",
    "    \"The future of technology is\",\n",
    "    \"In a world where AI\",\n",
    "    \"Machine learning algorithms\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    compare_generation_methods(prompt)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a5121",
   "metadata": {},
   "source": [
    "## 7. 🏋️‍♂️ Simulating LLM Training Steps\n",
    "\n",
    "Training Large Language Models involves two main phases that work together to create intelligent AI systems:\n",
    "\n",
    "### 📚 **Pre-training**: Learning from vast amounts of text\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "- Model learns to predict the next word in sentences\n",
    "- Trained on books, websites, articles (billions of words!)\n",
    "- Learns grammar, facts, reasoning patterns, and world knowledge\n",
    "\n",
    "**Key characteristics:**\n",
    "\n",
    "- **Self-supervised**: No human labeling needed, learns from raw text\n",
    "- **Massive scale**: Trillions of tokens from diverse sources\n",
    "- **General knowledge**: Absorbs broad understanding of language and concepts\n",
    "- **Foundation**: Creates the base intelligence that can be specialized later\n",
    "\n",
    "**Training objective:** Given \"The cat sat on the\", predict \"mat\"\n",
    "\n",
    "### 🎯 **Fine-tuning**: Specializing for specific tasks\n",
    "\n",
    "**What it does:**\n",
    "\n",
    "- Takes the pre-trained model and teaches it specific skills\n",
    "- Examples: answering questions, following instructions, being helpful\n",
    "- Uses smaller, high-quality datasets with human feedback\n",
    "\n",
    "**Key approaches:**\n",
    "\n",
    "- **Instruction tuning**: Teaching the model to follow human instructions\n",
    "- **RLHF**: Reinforcement Learning from Human Feedback for alignment\n",
    "- **Task-specific**: Fine-tuning for particular domains or use cases\n",
    "\n",
    "### The Two-Stage Strategy\n",
    "\n",
    "**Why this works:**\n",
    "\n",
    "1. **Pre-training** gives the model broad intelligence and language understanding\n",
    "2. **Fine-tuning** shapes this intelligence for specific, useful behaviors\n",
    "3. **Transfer learning**: Knowledge from pre-training transfers to specialized tasks\n",
    "4. **Efficiency**: Much cheaper than training from scratch for each task\n",
    "\n",
    "**Real-world scale:**\n",
    "\n",
    "- **Pre-training**: Weeks/months on thousands of GPUs, costs millions\n",
    "- **Fine-tuning**: Days/weeks on fewer GPUs, much more affordable\n",
    "\n",
    "Let's simulate these training steps to understand the process!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cecfe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Pre-training: Next Word Prediction\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SimplePretrainingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset for demonstrating pre-training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, tokenizer, max_length=32):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens = self.tokenizer.encode(text, max_length=self.max_length,\n",
    "                                       truncation=True, padding='max_length')\n",
    "\n",
    "        # For language modeling, input is tokens[:-1], target is tokens[1:]\n",
    "        input_ids = torch.tensor(tokens[:-1])\n",
    "        target_ids = torch.tensor(tokens[1:])\n",
    "        return input_ids, target_ids\n",
    "\n",
    "\n",
    "# Sample training data (in reality, this would be millions of texts)\n",
    "training_texts = [\n",
    "    \"Machine learning is transforming technology.\",\n",
    "    \"Neural networks learn patterns from data.\",\n",
    "    \"Transformers revolutionized natural language processing.\",\n",
    "    \"Artificial intelligence helps solve complex problems.\",\n",
    "    \"Deep learning models require large datasets.\",\n",
    "    \"Language models generate human-like text.\",\n",
    "    \"Attention mechanisms focus on relevant information.\",\n",
    "    \"Pre-training teaches models about language structure.\"\n",
    "]\n",
    "\n",
    "print(\"📊 Creating pre-training dataset...\")\n",
    "dataset = SimplePretrainingDataset(training_texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "def simulate_pretraining_step(model, batch, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Simulate one step of pre-training\n",
    "    \"\"\"\n",
    "    input_ids, target_ids = batch\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Calculate loss (how well model predicts next words)\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Set up small model for demonstration\n",
    "small_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "optimizer = optim.Adam(small_model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "print(\"\\n🏃‍♂️ Starting pre-training simulation...\")\n",
    "print(\"(In reality, this would run for days/weeks on thousands of GPUs!)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Simulate a few training steps\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        loss = simulate_pretraining_step(\n",
    "            small_model, batch, optimizer, criterion)\n",
    "        total_loss += loss\n",
    "\n",
    "        if batch_idx % 2 == 0:  # Print every 2 batches\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"📈 Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"✅ Pre-training simulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e189ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Fine-tuning: Teaching specific tasks\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Question-Answer dataset for fine-tuning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qa_pairs, tokenizer, max_length=64):\n",
    "        self.qa_pairs = qa_pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question, answer = self.qa_pairs[idx]\n",
    "\n",
    "        # Format as \"Q: [question] A: [answer]\"\n",
    "        text = f\"Q: {question} A: {answer}\"\n",
    "        tokens = self.tokenizer.encode(text, max_length=self.max_length,\n",
    "                                       truncation=True, padding='max_length')\n",
    "\n",
    "        input_ids = torch.tensor(tokens[:-1])\n",
    "        target_ids = torch.tensor(tokens[1:])\n",
    "        return input_ids, target_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qa_pairs)\n",
    "\n",
    "\n",
    "# Sample Q&A data for fine-tuning\n",
    "qa_data = [\n",
    "    (\"What is machine learning?\",\n",
    "     \"Machine learning is AI that learns patterns from data.\"),\n",
    "    (\"How do neural networks work?\",\n",
    "     \"Neural networks process information through connected layers.\"),\n",
    "    (\"What are transformers?\",\n",
    "     \"Transformers are models that use attention to process sequences.\"),\n",
    "    (\"Why is AI important?\", \"AI helps automate tasks and solve complex problems.\"),\n",
    "    (\"What is deep learning?\", \"Deep learning uses neural networks with multiple layers.\"),\n",
    "]\n",
    "\n",
    "print(\"🎯 Creating fine-tuning dataset...\")\n",
    "finetune_dataset = QADataset(qa_data, tokenizer)\n",
    "finetune_dataloader = DataLoader(finetune_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Use the model from pre-training for fine-tuning\n",
    "finetune_model = small_model  # In practice, you'd use a much larger pre-trained model\n",
    "finetune_optimizer = optim.Adam(\n",
    "    finetune_model.parameters(), lr=1e-5)  # Lower learning rate\n",
    "\n",
    "print(\"\\n🎓 Starting fine-tuning simulation...\")\n",
    "print(\"(Teaching the model to answer questions)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Fine-tuning steps\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(finetune_dataloader):\n",
    "        loss = simulate_pretraining_step(\n",
    "            finetune_model, batch, finetune_optimizer, criterion)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Show what the model is learning\n",
    "        question, answer = qa_data[batch_idx % len(qa_data)]\n",
    "        print(f\"Epoch {epoch+1}, Training on: Q: {question}\")\n",
    "        print(f\"   Expected Answer: {answer}\")\n",
    "        print(f\"   Loss: {loss:.4f}\\n\")\n",
    "\n",
    "    avg_loss = total_loss / len(finetune_dataloader)\n",
    "    print(f\"📈 Fine-tuning Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"✅ Fine-tuning simulation complete!\")\n",
    "print(\"🎉 Model has been pre-trained on language and fine-tuned for Q&A!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077ddf7",
   "metadata": {},
   "source": [
    "## 8. 💡 Prompt Engineering: Practical Examples\n",
    "\n",
    "**Prompt Engineering** is the art of crafting inputs to get the best outputs from language models. The way you ask determines what you get!\n",
    "\n",
    "### Why Prompt Engineering Matters\n",
    "\n",
    "LLMs are incredibly powerful, but they're also **sensitive to input phrasing**:\n",
    "\n",
    "- Small changes in wording can dramatically change outputs\n",
    "- The right prompt unlocks the model's full potential\n",
    "- Poor prompts lead to poor results, regardless of model capability\n",
    "\n",
    "### The Psychology of Prompting\n",
    "\n",
    "Think of prompting like **giving instructions to a very smart but literal assistant**:\n",
    "\n",
    "- Be specific about what you want\n",
    "- Provide context and examples when helpful\n",
    "- Set the right \"tone\" or \"role\" for the task\n",
    "- Break complex tasks into simpler steps\n",
    "\n",
    "### Key Prompt Engineering Techniques:\n",
    "\n",
    "1. **🎯 Clear Instructions**: Be specific about what you want\n",
    "\n",
    "   - ❌ Bad: \"Write about AI\"\n",
    "   - ✅ Good: \"Write a 200-word explanation of artificial intelligence for high school students\"\n",
    "\n",
    "2. **📝 Few-shot Learning**: Give examples in your prompt\n",
    "\n",
    "   - Show the model the pattern you want it to follow\n",
    "   - 2-3 examples usually work well\n",
    "\n",
    "3. **🎭 Role Playing**: Ask the model to act as an expert\n",
    "\n",
    "   - \"You are a senior software engineer...\"\n",
    "   - \"Act as a friendly teacher...\"\n",
    "\n",
    "4. **🧠 Chain of Thought**: Ask the model to think step by step\n",
    "\n",
    "   - \"Let's work through this step by step\"\n",
    "   - \"First, let me analyze... Then...\"\n",
    "\n",
    "5. **📋 Format Specification**: Tell the model exactly how to respond\n",
    "   - \"Respond in JSON format\"\n",
    "   - \"Use bullet points for your answer\"\n",
    "\n",
    "### Advanced Techniques\n",
    "\n",
    "- **Self-consistency**: Generate multiple answers and compare\n",
    "- **Constitutional prompting**: Give principles to follow\n",
    "- **Meta-prompting**: Prompts that help generate better prompts\n",
    "\n",
    "Let's explore these techniques with real examples!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dce4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Engineering Demonstration\n",
    "def test_prompt(prompt, model_name='gpt2', max_length=100):\n",
    "    \"\"\"\n",
    "    Test a prompt and return the generated response\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Prompt: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Encode and generate\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=len(input_ids[0]) + max_length,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Remove the original prompt from response\n",
    "    response = response[len(prompt):].strip()\n",
    "\n",
    "    print(f\"🤖 Response: {response}\")\n",
    "    print(\"=\" * 70)\n",
    "    return response\n",
    "\n",
    "\n",
    "# 1. Basic vs. Clear Instructions\n",
    "print(\"🔍 TECHNIQUE 1: Clear Instructions\")\n",
    "print()\n",
    "\n",
    "basic_prompt = \"Explain AI\"\n",
    "clear_prompt = \"Explain artificial intelligence in simple terms that a 12-year-old can understand. Include what it is, how it works, and give one real-world example.\"\n",
    "\n",
    "test_prompt(basic_prompt, max_length=50)\n",
    "test_prompt(clear_prompt, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Few-shot Learning Examples\n",
    "print(\"📚 TECHNIQUE 2: Few-shot Learning (Learning from Examples)\")\n",
    "print()\n",
    "\n",
    "few_shot_prompt = \"\"\"Classify the sentiment of these reviews:\n",
    "\n",
    "Review: \"This movie was absolutely amazing! I loved every minute.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"Terrible film, waste of time and money.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"It was okay, nothing special but not bad either.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Review: \"The new restaurant has incredible food and great service.\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "test_prompt(few_shot_prompt, max_length=20)\n",
    "\n",
    "# 3. Role Playing\n",
    "print(\"🎭 TECHNIQUE 3: Role Playing\")\n",
    "print()\n",
    "\n",
    "role_prompt = \"\"\"You are a friendly Python programming tutor. A student asks: \"I'm confused about loops in Python. Can you help?\"\n",
    "\n",
    "Respond as the tutor would:\"\"\"\n",
    "\n",
    "test_prompt(role_prompt, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Chain of Thought Reasoning\n",
    "print(\"🧠 TECHNIQUE 4: Chain of Thought\")\n",
    "print()\n",
    "\n",
    "chain_of_thought_prompt = \"\"\"Solve this step by step:\n",
    "\n",
    "Problem: A library has 150 books. On Monday, 23 books were borrowed and 7 were returned. On Tuesday, 18 books were borrowed and 12 were returned. How many books are in the library now?\n",
    "\n",
    "Let me think through this step by step:\n",
    "Step 1:\"\"\"\n",
    "\n",
    "test_prompt(chain_of_thought_prompt, max_length=100)\n",
    "\n",
    "# 5. Format Specification\n",
    "print(\"📋 TECHNIQUE 5: Format Specification\")\n",
    "print()\n",
    "\n",
    "format_prompt = \"\"\"Create a summary of machine learning. Format your response as:\n",
    "\n",
    "**Definition:** [One sentence definition]\n",
    "**Key Concepts:** [List 3 main concepts]\n",
    "**Applications:** [List 2 real-world uses]\n",
    "**Benefits:** [One key benefit]\n",
    "\n",
    "**Definition:**\"\"\"\n",
    "\n",
    "test_prompt(format_prompt, max_length=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d04984",
   "metadata": {},
   "source": [
    "## 9. 🔄 Comparing Prompting Techniques\n",
    "\n",
    "Let's compare different prompting strategies side by side to see how the approach affects the output quality!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Prompt Comparison\n",
    "def compare_prompts(task, prompts_dict, max_length=80):\n",
    "    \"\"\"\n",
    "    Compare different prompting approaches for the same task\n",
    "    \"\"\"\n",
    "    print(f\"🎯 Task: {task}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for technique, prompt in prompts_dict.items():\n",
    "        print(f\"\\n📋 {technique}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Generate response\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_length=len(input_ids[0]) + max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt):].strip()\n",
    "\n",
    "        print(f\"Response: {response}\")\n",
    "        results[technique] = response\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example 1: Explaining a technical concept\n",
    "task1 = \"Explain what neural networks are\"\n",
    "\n",
    "prompts1 = {\n",
    "    \"Basic\": \"What are neural networks?\",\n",
    "\n",
    "    \"Clear Instructions\": \"Explain neural networks in simple terms. Include what they are, how they work, and why they're useful. Use an analogy to help explain.\",\n",
    "\n",
    "    \"Role Playing\": \"You are a computer science professor. A student asks: 'Professor, can you explain neural networks in a way that's easy to understand?'\",\n",
    "\n",
    "    \"Few-shot with Examples\": \"\"\"Explain these AI concepts simply:\n",
    "\n",
    "Q: What is machine learning?\n",
    "A: Machine learning is like teaching a computer to recognize patterns, just like how you learn to recognize faces.\n",
    "\n",
    "Q: What are neural networks?\n",
    "A:\"\"\",\n",
    "\n",
    "    \"Chain of Thought\": \"Let me explain neural networks step by step: First, what they are, then how they work, then why they're important. Step 1 - What they are:\"\n",
    "}\n",
    "\n",
    "results1 = compare_prompts(task1, prompts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Problem-solving task\n",
    "task2 = \"Help with a coding problem\"\n",
    "\n",
    "prompts2 = {\n",
    "    \"Vague\": \"Help me with Python\",\n",
    "\n",
    "    \"Specific\": \"I'm getting an error 'list index out of range' in my Python code. Can you explain what this means and how to fix it?\",\n",
    "\n",
    "    \"With Context\": \"I'm a beginner programmer. I wrote Python code to access the 5th item in a list, but I get 'list index out of range' error. Please explain what's wrong and how to fix it, using simple terms.\",\n",
    "\n",
    "    \"Format Specified\": \"\"\"I have a Python error. Please help in this format:\n",
    "\n",
    "**Error Explanation:** [What the error means]\n",
    "**Common Causes:** [Why it happens]  \n",
    "**Solution:** [How to fix it]\n",
    "**Example:** [Show corrected code]\n",
    "\n",
    "My error: 'list index out of range'\n",
    "\n",
    "**Error Explanation:**\"\"\"\n",
    "}\n",
    "\n",
    "results2 = compare_prompts(task2, prompts2, max_length=100)\n",
    "\n",
    "# Interactive Exercise\n",
    "print(\"\\n🎮 INTERACTIVE EXERCISE\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Try crafting your own prompts! Here's a framework:\")\n",
    "print()\n",
    "print(\"📝 Template for effective prompts:\")\n",
    "print(\"   1. [Context/Role]: You are a [expert type]...\")\n",
    "print(\"   2. [Task]: I need you to [specific action]...\")\n",
    "print(\"   3. [Format]: Please respond with [specific format]...\")\n",
    "print(\"   4. [Examples]: Here are examples: [show examples]...\")\n",
    "print(\"   5. [Constraints]: Make sure to [specific requirements]...\")\n",
    "print()\n",
    "print(\"🎯 Try this yourself:\")\n",
    "print(\"   - Pick a topic you want to learn about\")\n",
    "print(\"   - Create both a 'bad' and 'good' prompt\")\n",
    "print(\"   - Test them and compare results!\")\n",
    "print()\n",
    "print(\"Example topics to try:\")\n",
    "print(\"   • Explain blockchain technology\")\n",
    "print(\"   • Help debug a programming error\")\n",
    "print(\"   • Summarize a research paper\")\n",
    "print(\"   • Create a learning plan\")\n",
    "print(\"   • Write creative content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b663af0",
   "metadata": {},
   "source": [
    "## 🎉 Conclusion & Summary\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through **Transformers, LLMs, and Prompt Engineering**!\n",
    "\n",
    "### 🎯 What You've Learned:\n",
    "\n",
    "#### **🏗️ Transformer Architecture**\n",
    "\n",
    "- ✅ Self-attention mechanism and how it works\n",
    "- ✅ Multi-head attention for parallel processing\n",
    "- ✅ Positional encoding and why it's needed\n",
    "- ✅ Encoder-decoder vs decoder-only architectures\n",
    "\n",
    "#### **🤖 Large Language Models (LLMs)**\n",
    "\n",
    "- ✅ How GPT uses transformers for text generation\n",
    "- ✅ Pre-training vs fine-tuning processes\n",
    "- ✅ Different generation strategies (greedy, sampling, top-k)\n",
    "- ✅ The training pipeline from raw text to helpful AI\n",
    "\n",
    "#### **💡 Prompt Engineering**\n",
    "\n",
    "- ✅ Clear instructions and specificity\n",
    "- ✅ Few-shot learning with examples\n",
    "- ✅ Role playing and persona assignment\n",
    "- ✅ Chain-of-thought reasoning\n",
    "- ✅ Format specification for structured outputs\n",
    "\n",
    "### 🚀 Key Takeaways:\n",
    "\n",
    "1. **Transformers revolutionized AI** by processing sequences in parallel and using attention\n",
    "2. **LLMs are prediction machines** that learn from massive amounts of text\n",
    "3. **Prompt engineering is crucial** - the way you ask determines what you get\n",
    "4. **Practice makes perfect** - experiment with different techniques!\n",
    "\n",
    "### 📚 Next Steps:\n",
    "\n",
    "- **🔬 Experiment**: Try different prompting techniques in your projects\n",
    "- **📖 Learn More**: Explore advanced topics like fine-tuning, RAG, and agents\n",
    "- **💻 Build**: Create your own applications using Hugging Face Transformers\n",
    "- **🤝 Share**: Help others understand these concepts!\n",
    "\n",
    "### 🔗 Useful Resources:\n",
    "\n",
    "- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "**Happy learning and building! 🎯🚀**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
