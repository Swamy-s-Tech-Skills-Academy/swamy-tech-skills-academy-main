{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b3e9b9",
   "metadata": {},
   "source": [
    "# üéØ Prompt Engineering: Practical Interactive Tutorial\n",
    "\n",
    "## üß† Session Overview\n",
    "\n",
    "**Prompting** is the art of crafting clear instructions for AI models using natural language.  \n",
    "It helps guide the model's reasoning, improve accuracy, and ensure structured and relevant responses.\n",
    "\n",
    "### Software/Tools Required\n",
    "\n",
    "> 1. **OS**: Windows 10/11 x64\n",
    "> 2. **Languages**: Python / .NET 8\n",
    "> 3. **IDEs**: Visual Studio 2022, Visual Studio Code\n",
    "> 4. **APIs**: OpenAI API (or mock function for demonstration)\n",
    "\n",
    "### Prior Knowledge\n",
    "\n",
    "> 1. Programming knowledge in C# / Python\n",
    "> 2. Basic understanding of AI/ML concepts\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "> 1. **.NET 8** for enterprise applications\n",
    "> 2. **AI & OpenAI APIs** for language model integration\n",
    "> 3. **Python** for experimentation and prototyping\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What We'll Learn Today\n",
    "\n",
    "1. **Understanding Prompt Engineering** fundamentals\n",
    "2. **Types of Prompting Techniques** with practical examples\n",
    "3. **Best Practices** for effective prompting\n",
    "4. **Real-world Applications** across different domains\n",
    "5. **Performance Optimization** and cost considerations\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Types of Prompting Techniques\n",
    "\n",
    "| Technique            | Use Case              | Complexity |\n",
    "| -------------------- | --------------------- | ---------- |\n",
    "| **Single-Turn**      | Simple tasks          | ‚≠ê         |\n",
    "| **Zero-Shot**        | General queries       | ‚≠ê‚≠ê       |\n",
    "| **Few-Shot**         | Pattern learning      | ‚≠ê‚≠ê‚≠ê     |\n",
    "| **Multi-Turn**       | Conversations         | ‚≠ê‚≠ê‚≠ê     |\n",
    "| **Role-Based**       | Specialized responses | ‚≠ê‚≠ê‚≠ê     |\n",
    "| **Chain-of-Thought** | Complex reasoning     | ‚≠ê‚≠ê‚≠ê‚≠ê   |\n",
    "| **Prompt Chaining**  | Workflow automation   | ‚≠ê‚≠ê‚≠ê‚≠ê   |\n",
    "| **Self-Consistency** | Verification          | ‚≠ê‚≠ê‚≠ê‚≠ê   |\n",
    "| **Conversational**   | AI assistants         | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103748f",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries for our prompt engineering demonstration. We'll use both OpenAI's API and create mock functions for demonstration purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326dc370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# For OpenAI API (install with: pip install openai)\n",
    "try:\n",
    "    import openai\n",
    "    HAS_OPENAI = True\n",
    "    print(\"‚úÖ OpenAI library available\")\n",
    "except ImportError:\n",
    "    HAS_OPENAI = False\n",
    "    print(\"‚ö†Ô∏è OpenAI library not installed. Using mock responses.\")\n",
    "\n",
    "# Configuration\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptConfig:\n",
    "    model: str = \"gpt-3.5-turbo\"\n",
    "    max_tokens: int = 500\n",
    "    temperature: float = 0.7\n",
    "\n",
    "\n",
    "config = PromptConfig()\n",
    "\n",
    "# Mock AI Response Function (for demonstration without API key)\n",
    "\n",
    "\n",
    "def mock_ai_response(prompt: str, role: str = \"assistant\") -> str:\n",
    "    \"\"\"\n",
    "    Mock function to simulate AI responses for demonstration purposes.\n",
    "    In production, replace this with actual OpenAI API calls.\n",
    "    \"\"\"\n",
    "    responses = {\n",
    "        \"translation\": \"Hello\",\n",
    "        \"summary\": \"AI is about creating intelligent systems that can perform human-like tasks.\",\n",
    "        \"math\": \"36\",\n",
    "        \"reasoning\": \"Let me think step by step: 10 - 4 - 2 = 4 apples remaining\",\n",
    "        \"code_review\": \"The code looks good overall. Consider adding error handling for edge cases.\",\n",
    "        \"conversation\": \"I understand. How can I help you with that?\",\n",
    "        \"analysis\": \"Based on the data provided, I can see several interesting patterns...\"\n",
    "    }\n",
    "\n",
    "    # Simple keyword matching for demo\n",
    "    prompt_lower = prompt.lower()\n",
    "    if \"translate\" in prompt_lower:\n",
    "        return responses[\"translation\"]\n",
    "    elif \"summarize\" in prompt_lower or \"summary\" in prompt_lower:\n",
    "        return responses[\"summary\"]\n",
    "    elif any(word in prompt_lower for word in [\"calculate\", \"math\", \"percentage\"]):\n",
    "        return responses[\"math\"]\n",
    "    elif \"step\" in prompt_lower or \"reasoning\" in prompt_lower:\n",
    "        return responses[\"reasoning\"]\n",
    "    elif \"review\" in prompt_lower or \"code\" in prompt_lower:\n",
    "        return responses[\"code_review\"]\n",
    "    elif \"analyze\" in prompt_lower or \"analysis\" in prompt_lower:\n",
    "        return responses[\"analysis\"]\n",
    "    else:\n",
    "        return responses[\"conversation\"]\n",
    "\n",
    "# Real OpenAI API Function (requires API key)\n",
    "\n",
    "\n",
    "def openai_response(prompt: str, role: str = \"user\") -> str:\n",
    "    \"\"\"\n",
    "    Real OpenAI API call. Requires OPENAI_API_KEY environment variable.\n",
    "    \"\"\"\n",
    "    if not HAS_OPENAI:\n",
    "        return mock_ai_response(prompt, role)\n",
    "\n",
    "    try:\n",
    "        # Set your OpenAI API key\n",
    "        # openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        # For demonstration, we'll use mock responses\n",
    "        # Uncomment the following lines to use real API:\n",
    "\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=config.model,\n",
    "        #     messages=[{\"role\": role, \"content\": prompt}],\n",
    "        #     max_tokens=config.max_tokens,\n",
    "        #     temperature=config.temperature\n",
    "        # )\n",
    "        # return response.choices[0].message.content.strip()\n",
    "\n",
    "        # Using mock for demo\n",
    "        return mock_ai_response(prompt, role)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        return mock_ai_response(prompt, role)\n",
    "\n",
    "# Helper function to display responses nicely\n",
    "\n",
    "\n",
    "def display_prompt_response(prompt: str, response: str, technique: str = \"\"):\n",
    "    \"\"\"Display prompt and response in a formatted way\"\"\"\n",
    "    print(f\"üéØ {technique}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìù Prompt:\")\n",
    "    print(f\"   {prompt}\")\n",
    "    print()\n",
    "    print(f\"ü§ñ Response:\")\n",
    "    print(f\"   {response}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\"üöÄ Setup complete! Ready to explore prompt engineering techniques.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3005ef",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Single-Turn Prompting\n",
    "\n",
    "**Definition:**  \n",
    "A basic one-shot interaction where a single prompt yields a single response.  \n",
    "Useful for simple tasks without context.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- ‚úÖ Simple and straightforward\n",
    "- ‚úÖ Fast execution\n",
    "- ‚úÖ Good for basic tasks\n",
    "- ‚ùå Limited context\n",
    "- ‚ùå No conversation memory\n",
    "\n",
    "**Best for:** Translation, simple questions, basic transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-Turn Prompting Examples\n",
    "\n",
    "def single_turn_example():\n",
    "    \"\"\"Demonstrate single-turn prompting with various examples\"\"\"\n",
    "\n",
    "    examples = [\n",
    "        \"Translate 'Bonjour' to English.\",\n",
    "        \"What is the capital of Japan?\",\n",
    "        \"Convert 25¬∞C to Fahrenheit.\",\n",
    "        \"Define 'machine learning' in one sentence.\"\n",
    "    ]\n",
    "\n",
    "    print(\"üéØ SINGLE-TURN PROMPTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, prompt in enumerate(examples, 1):\n",
    "        print(f\"\\nüìù Example {i}:\")\n",
    "        response = openai_response(prompt)\n",
    "        display_prompt_response(prompt, response, f\"Single-Turn Example {i}\")\n",
    "\n",
    "\n",
    "# Run the examples\n",
    "single_turn_example()\n",
    "\n",
    "# Interactive single-turn prompt\n",
    "print(\"üîß TRY YOUR OWN SINGLE-TURN PROMPT:\")\n",
    "print(\"Uncomment the lines below to try your own prompt\")\n",
    "\n",
    "# user_prompt = input(\"Enter your single-turn prompt: \")\n",
    "# if user_prompt:\n",
    "#     response = openai_response(user_prompt)\n",
    "#     display_prompt_response(user_prompt, response, \"Your Single-Turn Prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575141fe",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Zero-Shot Prompting\n",
    "\n",
    "**Definition:**  \n",
    "Ask the model to perform a task without providing any examples.  \n",
    "Best for general-purpose queries the model is likely pre-trained on.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- ‚úÖ No examples needed\n",
    "- ‚úÖ Works for many common tasks\n",
    "- ‚úÖ Quick and simple\n",
    "- ‚ùå May not understand specific formats\n",
    "- ‚ùå Performance varies by task complexity\n",
    "- ‚ùå Less reliable for specialized domains\n",
    "\n",
    "**When to use:**\n",
    "\n",
    "- Tasks the model is well-trained on\n",
    "- General knowledge questions\n",
    "- Common text transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87068ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot Prompting Examples\n",
    "\n",
    "def zero_shot_example():\n",
    "    \"\"\"Demonstrate zero-shot prompting across different domains\"\"\"\n",
    "\n",
    "    examples = [\n",
    "        {\n",
    "            \"task\": \"Text Summarization\",\n",
    "            \"prompt\": \"Summarize this text: 'Artificial intelligence is a field of computer science focused on building systems that can perform tasks typically requiring human intelligence, such as visual perception, speech recognition, decision-making, and language translation.'\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Sentiment Analysis\",\n",
    "            \"prompt\": \"Classify the sentiment of this review as positive, negative, or neutral: 'The product arrived quickly and works exactly as described. Very satisfied with the purchase.'\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Question Answering\",\n",
    "            \"prompt\": \"Answer this question based on common knowledge: 'What are the main benefits of using renewable energy sources?'\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Text Classification\",\n",
    "            \"prompt\": \"Classify this email as spam or not spam: 'Congratulations! You have won $1,000,000! Click here immediately to claim your prize before it expires!'\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"üéØ ZERO-SHOT PROMPTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nüìù Task {i}: {example['task']}\")\n",
    "        response = openai_response(example['prompt'])\n",
    "        display_prompt_response(\n",
    "            example['prompt'], response, f\"Zero-Shot: {example['task']}\")\n",
    "\n",
    "\n",
    "# Run the examples\n",
    "zero_shot_example()\n",
    "\n",
    "# Zero-shot vs. poorly structured comparison\n",
    "print(\"üîç COMPARISON: Good vs. Poor Zero-Shot Prompts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Poor example\n",
    "poor_prompt = \"Analyze this\"\n",
    "poor_response = openai_response(poor_prompt)\n",
    "display_prompt_response(poor_prompt, poor_response,\n",
    "                        \"‚ùå Poor Zero-Shot (Too Vague)\")\n",
    "\n",
    "# Good example\n",
    "good_prompt = \"Analyze the writing style of this text and identify the target audience: 'Hey there! Ready to boost your productivity? Our new app makes task management super easy with drag-and-drop features and smart notifications.'\"\n",
    "good_response = openai_response(good_prompt)\n",
    "display_prompt_response(good_prompt, good_response,\n",
    "                        \"‚úÖ Good Zero-Shot (Clear & Specific)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba4546f",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Few-Shot Prompting\n",
    "\n",
    "**Definition:**  \n",
    "Provide 2‚Äì3 examples to show what kind of answer is expected.  \n",
    "Helps the model learn the desired pattern and format.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- ‚úÖ Better accuracy than zero-shot\n",
    "- ‚úÖ Teaches format and style\n",
    "- ‚úÖ Good for pattern recognition\n",
    "- ‚ùå Requires good examples\n",
    "- ‚ùå Longer prompts (more tokens)\n",
    "- ‚ùå Examples must be representative\n",
    "\n",
    "**Key Success Factors:**\n",
    "\n",
    "1. **Quality Examples**: Choose diverse, representative examples\n",
    "2. **Consistent Format**: Maintain the same structure across examples\n",
    "3. **Clear Pattern**: Make the desired output pattern obvious\n",
    "4. **Appropriate Quantity**: Usually 2-5 examples work best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c46d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Prompting Examples\n",
    "\n",
    "def few_shot_example():\n",
    "    \"\"\"Demonstrate few-shot prompting with various patterns\"\"\"\n",
    "\n",
    "    # Example 1: Text Classification with Format Learning\n",
    "    classification_prompt = \"\"\"\n",
    "Classify the following text as Technical, Business, or Personal:\n",
    "\n",
    "Text: \"Our server response time has increased to 2.3 seconds due to database query optimization issues.\"\n",
    "Classification: Technical\n",
    "\n",
    "Text: \"The quarterly revenue exceeded projections by 15% thanks to strong sales in the mobile app segment.\"\n",
    "Classification: Business\n",
    "\n",
    "Text: \"I'm planning a weekend trip to the mountains and need to pack my hiking gear.\"\n",
    "Classification: Personal\n",
    "\n",
    "Text: \"The new machine learning model achieved 94% accuracy on the validation dataset.\"\n",
    "Classification: \"\"\"\n",
    "\n",
    "    # Example 2: Format Conversion\n",
    "    format_conversion_prompt = \"\"\"\n",
    "Convert the following phrases to PascalCase:\n",
    "\n",
    "Input: \"user authentication service\"\n",
    "Output: UserAuthenticationService\n",
    "\n",
    "Input: \"payment processing module\"\n",
    "Output: PaymentProcessingModule\n",
    "\n",
    "Input: \"customer data repository\"\n",
    "Output: CustomerDataRepository\n",
    "\n",
    "Input: \"email notification system\"\n",
    "Output: \"\"\"\n",
    "\n",
    "    # Example 3: Structured Data Extraction\n",
    "    data_extraction_prompt = \"\"\"\n",
    "Extract key information from the following product descriptions in JSON format:\n",
    "\n",
    "Description: \"iPhone 14 Pro - 128GB storage, 6.1-inch display, available in Space Black, priced at $999\"\n",
    "JSON: {\"product\": \"iPhone 14 Pro\", \"storage\": \"128GB\", \"display\": \"6.1-inch\", \"color\": \"Space Black\", \"price\": \"$999\"}\n",
    "\n",
    "Description: \"MacBook Air M2 - 256GB SSD, 13.6-inch Retina display, available in Midnight, priced at $1199\"\n",
    "JSON: {\"product\": \"MacBook Air M2\", \"storage\": \"256GB SSD\", \"display\": \"13.6-inch Retina\", \"color\": \"Midnight\", \"price\": \"$1199\"}\n",
    "\n",
    "Description: \"Samsung Galaxy S23 - 512GB storage, 6.8-inch AMOLED display, available in Phantom Black, priced at $1199\"\n",
    "JSON: \"\"\"\n",
    "\n",
    "    examples = [\n",
    "        (\"Text Classification\", classification_prompt),\n",
    "        (\"Format Conversion\", format_conversion_prompt),\n",
    "        (\"Data Extraction\", data_extraction_prompt)\n",
    "    ]\n",
    "\n",
    "    print(\"üéØ FEW-SHOT PROMPTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, (task, prompt) in enumerate(examples, 1):\n",
    "        print(f\"\\nüìù Example {i}: {task}\")\n",
    "        response = openai_response(prompt)\n",
    "        display_prompt_response(\n",
    "            prompt[:200] + \"...\", response, f\"Few-Shot: {task}\")\n",
    "\n",
    "\n",
    "# Run the examples\n",
    "few_shot_example()\n",
    "\n",
    "# Comparison: Bad vs Good Few-Shot Examples\n",
    "print(\"üîç COMPARISON: Bad vs. Good Few-Shot Prompts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bad few-shot example (inconsistent format)\n",
    "bad_few_shot = \"\"\"\n",
    "data science -> DataScience\n",
    "student login -> \n",
    "account settings -> AccountSettings\n",
    "user profile ->\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ùå BAD FEW-SHOT EXAMPLE:\")\n",
    "print(\"Issues: Inconsistent format, missing examples, unclear pattern\")\n",
    "print(f\"Prompt: {bad_few_shot}\")\n",
    "print()\n",
    "\n",
    "# Good few-shot example (consistent format)\n",
    "good_few_shot = \"\"\"\n",
    "Convert to PascalCase:\n",
    "\n",
    "Q: Convert 'data science' to PascalCase\n",
    "A: DataScience\n",
    "\n",
    "Q: Convert 'student login' to PascalCase  \n",
    "A: StudentLogin\n",
    "\n",
    "Q: Convert 'account settings' to PascalCase\n",
    "A: AccountSettings\n",
    "\n",
    "Q: Convert 'user profile' to PascalCase\n",
    "A: \"\"\"\n",
    "\n",
    "print(\"‚úÖ GOOD FEW-SHOT EXAMPLE:\")\n",
    "print(\"Strengths: Consistent Q/A format, clear pattern, complete examples\")\n",
    "good_response = openai_response(good_few_shot)\n",
    "display_prompt_response(good_few_shot, good_response, \"Good Few-Shot Format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0217e29",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Multi-Turn Prompting\n",
    "\n",
    "**Definition:**  \n",
    "A conversation-like interaction with multiple back-and-forth prompts.  \n",
    "Ideal for scenarios requiring memory of prior context.\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- ‚úÖ Maintains conversation context\n",
    "- ‚úÖ Builds on previous responses\n",
    "- ‚úÖ Natural dialogue flow\n",
    "- ‚ùå Requires context management\n",
    "- ‚ùå Higher token consumption\n",
    "- ‚ùå Context window limitations\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "- Customer support conversations\n",
    "- Educational tutoring sessions\n",
    "- Complex problem-solving\n",
    "- Interactive consultations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Turn Prompting Examples\n",
    "\n",
    "class ConversationManager:\n",
    "    \"\"\"Manages multi-turn conversations with context\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"Add a message to the conversation history\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def get_context(self) -> str:\n",
    "        \"\"\"Get the full conversation context\"\"\"\n",
    "        context = \"\"\n",
    "        for message in self.conversation_history:\n",
    "            context += f\"{message['role'].title()}: {message['content']}\\n\"\n",
    "        return context\n",
    "\n",
    "    def send_message(self, user_message: str) -> str:\n",
    "        \"\"\"Send a message and get AI response with full context\"\"\"\n",
    "        self.add_message(\"user\", user_message)\n",
    "\n",
    "        # Create context-aware prompt\n",
    "        context_prompt = self.get_context() + \"Assistant: \"\n",
    "\n",
    "        response = openai_response(context_prompt)\n",
    "        self.add_message(\"assistant\", response)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def display_conversation(self):\n",
    "        \"\"\"Display the full conversation\"\"\"\n",
    "        print(\"üí¨ CONVERSATION HISTORY:\")\n",
    "        print(\"-\" * 40)\n",
    "        for message in self.conversation_history:\n",
    "            role_emoji = \"üë§\" if message[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "            print(\n",
    "                f\"{role_emoji} {message['role'].title()}: {message['content']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "def multi_turn_example():\n",
    "    \"\"\"Demonstrate multi-turn prompting scenarios\"\"\"\n",
    "\n",
    "    print(\"üéØ MULTI-TURN PROMPTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Example 1: Travel Planning Conversation\n",
    "    print(\"\\nüìù Scenario 1: Travel Planning Assistant\")\n",
    "    travel_conv = ConversationManager()\n",
    "\n",
    "    # Turn 1\n",
    "    response1 = travel_conv.send_message(\"What's the capital of France?\")\n",
    "    print(f\"User: What's the capital of France?\")\n",
    "    print(f\"AI: {response1}\")\n",
    "\n",
    "    # Turn 2 (builds on previous context)\n",
    "    response2 = travel_conv.send_message(\"How far is it from Berlin?\")\n",
    "    print(f\"User: How far is it from Berlin?\")\n",
    "    print(f\"AI: {response2}\")\n",
    "\n",
    "    # Turn 3 (continues the conversation)\n",
    "    response3 = travel_conv.send_message(\n",
    "        \"What's the best way to travel between these cities?\")\n",
    "    print(f\"User: What's the best way to travel between these cities?\")\n",
    "    print(f\"AI: {response3}\")\n",
    "\n",
    "    travel_conv.display_conversation()\n",
    "\n",
    "    # Example 2: Technical Support Conversation\n",
    "    print(\"\\nüìù Scenario 2: Technical Support\")\n",
    "    support_conv = ConversationManager()\n",
    "\n",
    "    turns = [\n",
    "        \"My application is running slowly\",\n",
    "        \"It's a web application built with React\",\n",
    "        \"The slowness happens when users try to load the dashboard\",\n",
    "        \"Yes, it started after we deployed the latest update yesterday\"\n",
    "    ]\n",
    "\n",
    "    ai_responses = [\n",
    "        \"I can help you troubleshoot the performance issue. Can you tell me what type of application you're working with?\",\n",
    "        \"React applications can have various performance bottlenecks. When specifically does the slowness occur?\",\n",
    "        \"Dashboard loading issues are common. Have you noticed if this started recently or after any specific changes?\",\n",
    "        \"That's helpful context. Recent deployments often introduce performance regressions. Let's check a few things: 1) Browser network tab for slow requests, 2) Console for JavaScript errors, 3) Any new dependencies added in the latest update.\"\n",
    "    ]\n",
    "\n",
    "    for turn, ai_response in zip(turns, ai_responses):\n",
    "        print(f\"User: {turn}\")\n",
    "        # For demo, we'll use predefined responses that show context awareness\n",
    "        support_conv.add_message(\"user\", turn)\n",
    "        support_conv.add_message(\"assistant\", ai_response)\n",
    "        print(f\"AI: {ai_response}\")\n",
    "        print()\n",
    "\n",
    "    support_conv.display_conversation()\n",
    "\n",
    "\n",
    "# Run the examples\n",
    "multi_turn_example()\n",
    "\n",
    "# Interactive multi-turn conversation\n",
    "print(\"üîß INTERACTIVE MULTI-TURN DEMO:\")\n",
    "print(\"Uncomment the code below to try your own multi-turn conversation\")\n",
    "\n",
    "# conversation = ConversationManager()\n",
    "# print(\"Start a conversation! Type 'quit' to end.\")\n",
    "#\n",
    "# while True:\n",
    "#     user_input = input(\"You: \")\n",
    "#     if user_input.lower() == 'quit':\n",
    "#         break\n",
    "#\n",
    "#     response = conversation.send_message(user_input)\n",
    "#     print(f\"AI: {response}\")\n",
    "#     print()\n",
    "#\n",
    "# conversation.display_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97669c2b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Role-Based Prompting\n",
    "\n",
    "**Definition:**  \n",
    "Assign a persona or role to guide the tone, detail, or depth of the answer.  \n",
    "Useful in simulations, teaching, or reviews.\n",
    "\n",
    "**Effective Roles:**\n",
    "\n",
    "- **Expert Consultant**: \"As a senior data scientist...\"\n",
    "- **Teacher**: \"Explain this as if teaching a beginner...\"\n",
    "- **Analyst**: \"From a business analysis perspective...\"\n",
    "- **Critic**: \"Critically evaluate this approach...\"\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "- ‚úÖ Controls response style and depth\n",
    "- ‚úÖ Provides domain expertise context\n",
    "- ‚úÖ Sets appropriate technical level\n",
    "- ‚úÖ Improves response relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role-Based Prompting Examples\n",
    "\n",
    "def role_based_example():\n",
    "    \"\"\"Demonstrate role-based prompting with different personas\"\"\"\n",
    "\n",
    "    base_content = \"Review this code: def calculate_total(items): return sum([item.price * item.quantity for item in items])\"\n",
    "\n",
    "    role_examples = [\n",
    "        {\n",
    "            \"role\": \"Senior Software Engineer\",\n",
    "            \"prompt\": f\"You are a senior software engineer with 10 years of experience. {base_content}\",\n",
    "            \"focus\": \"Code quality, best practices, and maintainability\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Security Expert\",\n",
    "            \"prompt\": f\"You are a cybersecurity expert specializing in secure coding practices. {base_content}\",\n",
    "            \"focus\": \"Security vulnerabilities and safe coding practices\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Performance Consultant\",\n",
    "            \"prompt\": f\"You are a performance optimization consultant. {base_content}\",\n",
    "            \"focus\": \"Performance implications and optimization opportunities\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"Beginner Mentor\",\n",
    "            \"prompt\": f\"You are a patient mentor teaching programming to beginners. Explain this code in simple terms: {base_content}\",\n",
    "            \"focus\": \"Educational explanation for beginners\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"üéØ ROLE-BASED PROMPTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, example in enumerate(role_examples, 1):\n",
    "        print(f\"\\nüìù Role {i}: {example['role']}\")\n",
    "        print(f\"Focus: {example['focus']}\")\n",
    "        response = openai_response(example['prompt'])\n",
    "        display_prompt_response(\n",
    "            example['prompt'], response, f\"Role: {example['role']}\")\n",
    "\n",
    "\n",
    "# Run role-based examples\n",
    "role_based_example()\n",
    "\n",
    "# Comparison: Vague vs. Specific Role\n",
    "print(\"üîç COMPARISON: Vague vs. Specific Role Assignment\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Vague role\n",
    "vague_prompt = \"Review this code.\"\n",
    "vague_response = openai_response(vague_prompt)\n",
    "display_prompt_response(vague_prompt, vague_response, \"‚ùå Vague (No Role)\")\n",
    "\n",
    "# Specific role\n",
    "specific_prompt = \"You are a senior software architect with expertise in distributed systems and 15 years of experience. Review this microservice API design for scalability, maintainability, and best practices: [API design here]\"\n",
    "specific_response = openai_response(specific_prompt)\n",
    "display_prompt_response(\n",
    "    specific_prompt[:100] + \"...\", specific_response, \"‚úÖ Specific Role (Detailed Persona)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147c47b",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Chain-of-Thought Prompting\n",
    "\n",
    "**Definition:**  \n",
    "Encourages the model to reason step-by-step before answering.  \n",
    "Improves performance on math, logic, and planning tasks.\n",
    "\n",
    "**Key Phrases:**\n",
    "\n",
    "- \"Let me think step by step\"\n",
    "- \"Let's work through this systematically\"\n",
    "- \"I'll break this down into parts\"\n",
    "- \"First, let me analyze...\"\n",
    "\n",
    "**Best for:**\n",
    "\n",
    "- Mathematical problems\n",
    "- Logical reasoning\n",
    "- Complex analysis\n",
    "- Problem-solving tasks\n",
    "- Decision-making processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought Prompting Examples\n",
    "\n",
    "def chain_of_thought_example():\n",
    "    \"\"\"Demonstrate chain-of-thought reasoning across different domains\"\"\"\n",
    "\n",
    "    examples = [\n",
    "        {\n",
    "            \"category\": \"Mathematics\",\n",
    "            \"prompt\": \"Q: There are 10 apples in a basket. Alice eats 4 apples, Bob eats 2 apples. How many apples are left?\\nLet me think step by step:\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Logic\",\n",
    "            \"prompt\": \"Q: If all roses are flowers, and all flowers need water, and I have a rose, what can I conclude?\\nLet me work through this logically:\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Business Analysis\",\n",
    "            \"prompt\": \"Q: A company's revenue increased 20% but profits decreased 5%. What might explain this situation?\\nLet me analyze this step by step:\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Programming\",\n",
    "            \"prompt\": \"Q: Why might a database query be running slowly?\\nLet me think through the potential causes systematically:\"\n",
    "        },\n",
    "        {\n",
    "            \"category\": \"Problem Solving\",\n",
    "            \"prompt\": \"Q: Calculate 15% of 240 and show multiple approaches.\\nLet me solve this in different ways:\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"üéØ CHAIN-OF-THOUGHT PROMPTING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\nüìù Example {i}: {example['category']}\")\n",
    "        response = openai_response(example['prompt'])\n",
    "        display_prompt_response(\n",
    "            example['prompt'], response, f\"Chain-of-Thought: {example['category']}\")\n",
    "\n",
    "\n",
    "# Run examples\n",
    "chain_of_thought_example()\n",
    "\n",
    "# Comparison: Direct vs. Chain-of-Thought\n",
    "print(\"üîç COMPARISON: Direct Answer vs. Chain-of-Thought\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Direct answer\n",
    "direct_prompt = \"What's 25% of 160?\"\n",
    "direct_response = openai_response(direct_prompt)\n",
    "display_prompt_response(direct_prompt, direct_response,\n",
    "                        \"‚ùå Direct (No Reasoning)\")\n",
    "\n",
    "# Chain-of-thought\n",
    "cot_prompt = \"What's 25% of 160? Let me think step by step and show my work.\"\n",
    "cot_response = openai_response(cot_prompt)\n",
    "display_prompt_response(cot_prompt, cot_response,\n",
    "                        \"‚úÖ Chain-of-Thought (With Reasoning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd280931",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Advanced Techniques Summary\n",
    "\n",
    "This section demonstrates the remaining advanced prompting techniques in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25351bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Prompting Techniques\n",
    "\n",
    "def prompt_chaining_example():\n",
    "    \"\"\"Demonstrate prompt chaining for complex workflows\"\"\"\n",
    "\n",
    "    print(\"üéØ PROMPT CHAINING EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Scenario: Creating a complete bug report from user feedback\")\n",
    "\n",
    "    # Step 1: Analyze the issue\n",
    "    user_feedback = \"The login button doesn't work on Safari browser. When I click it, nothing happens.\"\n",
    "\n",
    "    step1_prompt = f\"Analyze this user feedback and extract the key technical details: '{user_feedback}'\"\n",
    "    step1_response = openai_response(step1_prompt)\n",
    "    print(f\"Step 1 - Issue Analysis:\")\n",
    "    print(f\"Input: {user_feedback}\")\n",
    "    print(f\"Output: {step1_response}\")\n",
    "    print()\n",
    "\n",
    "    # Step 2: Create technical summary\n",
    "    step2_prompt = f\"Based on this analysis: '{step1_response}', create a concise technical summary suitable for a bug report.\"\n",
    "    step2_response = openai_response(step2_prompt)\n",
    "    print(f\"Step 2 - Technical Summary:\")\n",
    "    print(f\"Output: {step2_response}\")\n",
    "    print()\n",
    "\n",
    "    # Step 3: Generate action items\n",
    "    step3_prompt = f\"Based on this bug report summary: '{step2_response}', generate 3-5 specific action items for developers to investigate.\"\n",
    "    step3_response = openai_response(step3_prompt)\n",
    "    print(f\"Step 3 - Action Items:\")\n",
    "    print(f\"Output: {step3_response}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def self_consistency_example():\n",
    "    \"\"\"Demonstrate self-consistency prompting\"\"\"\n",
    "\n",
    "    print(\"\\nüéØ SELF-CONSISTENCY PROMPTING EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Scenario: Solving a problem multiple ways to verify consistency\")\n",
    "\n",
    "    problem = \"Calculate 15% of 240\"\n",
    "    approaches = [\n",
    "        f\"{problem}. Method 1: Use decimal multiplication.\",\n",
    "        f\"{problem}. Method 2: Use fraction calculation.\",\n",
    "        f\"{problem}. Method 3: Use percentage formula.\",\n",
    "        f\"{problem}. Method 4: Use proportional reasoning.\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for i, approach in enumerate(approaches, 1):\n",
    "        response = openai_response(approach)\n",
    "        results.append(response)\n",
    "        print(f\"Approach {i}: {response}\")\n",
    "\n",
    "    print(f\"\\nConsistency check: All methods should give the same answer.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def conversational_prompting_example():\n",
    "    \"\"\"Demonstrate conversational prompting with memory and role adaptation\"\"\"\n",
    "\n",
    "    print(\"\\nüéØ CONVERSATIONAL PROMPTING EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Scenario: AI Shopping Assistant with memory\")\n",
    "\n",
    "    # Initialize conversation with role\n",
    "    shopping_assistant = ConversationManager()\n",
    "    shopping_assistant.add_message(\n",
    "        \"system\", \"You are a helpful shopping assistant. Keep track of items users mention and provide personalized recommendations.\")\n",
    "\n",
    "    # Conversation flow\n",
    "    conversation_turns = [\n",
    "        (\"I need to buy some bananas\", \"I'll help you with your shopping! Bananas are a great choice - they're nutritious and versatile. How many do you need? I'll keep track of your shopping list.\"),\n",
    "        (\"Add milk and eggs too\", \"Perfect! I've added milk and eggs to your list. So far you have:\\n1. Bananas\\n2. Milk\\n3. Eggs\\n\\nAre you planning to do some baking? These are common baking ingredients.\"),\n",
    "        (\"Yes, I want to make pancakes\", \"Excellent! For pancakes, you might also want to consider:\\n- Flour (if you don't have it)\\n- Sugar\\n- Baking powder\\n- Butter\\n\\nYour current list works great for pancakes. Would you like me to suggest quantities or recipes?\"),\n",
    "        (\"What's a simple pancake recipe?\", \"Here's a simple pancake recipe using your ingredients:\\n\\n**Basic Pancakes:**\\n- 1 cup flour\\n- 1 egg\\n- 1 cup milk\\n- 1 banana (mashed)\\n- 2 tbsp sugar\\n- 1 tsp baking powder\\n\\nMix dry ingredients, then add wet ingredients. Cook on medium heat. The banana will add natural sweetness!\")\n",
    "    ]\n",
    "\n",
    "    for user_msg, ai_response in conversation_turns:\n",
    "        shopping_assistant.add_message(\"user\", user_msg)\n",
    "        shopping_assistant.add_message(\"assistant\", ai_response)\n",
    "        print(f\"User: {user_msg}\")\n",
    "        print(f\"AI: {ai_response}\")\n",
    "        print()\n",
    "\n",
    "    shopping_assistant.display_conversation()\n",
    "\n",
    "\n",
    "# Run advanced examples\n",
    "prompt_chaining_example()\n",
    "self_consistency_example()\n",
    "conversational_prompting_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0277b1",
   "metadata": {},
   "source": [
    "## ‚úÖ Best Practices Demonstration\n",
    "\n",
    "Let's implement the key best practices for effective prompt engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcbbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practices Implementation\n",
    "\n",
    "def demonstrate_best_practices():\n",
    "    \"\"\"Show practical implementation of prompt engineering best practices\"\"\"\n",
    "\n",
    "    print(\"üéØ PROMPT ENGINEERING BEST PRACTICES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Practice 1: Be Clear & Specific\n",
    "    print(\"\\n1Ô∏è‚É£ BE CLEAR & SPECIFIC\")\n",
    "    vague = \"Make this better\"\n",
    "    specific = \"Improve this Python function's readability by adding docstrings, type hints, and descriptive variable names: def calc(x, y): return x * 0.15 + y\"\n",
    "\n",
    "    print(f\"‚ùå Vague: {vague}\")\n",
    "    print(f\"‚úÖ Specific: {specific[:80]}...\")\n",
    "\n",
    "    # Practice 2: Add Context & Examples\n",
    "    print(\"\\n2Ô∏è‚É£ ADD CONTEXT & EXAMPLES\")\n",
    "    no_context = \"Format this data\"\n",
    "    with_context = \"\"\"Format this sales data into a professional report:\n",
    "    \n",
    "Data: Q1: $125K, Q2: $140K, Q3: $160K, Q4: $180K\n",
    "\n",
    "Example format:\n",
    "## Quarterly Sales Report\n",
    "**Q1 Performance**: $125,000 (baseline)\n",
    "**Growth Analysis**: [analysis here]\n",
    "\n",
    "Please follow this format for all quarters.\"\"\"\n",
    "\n",
    "    print(f\"‚ùå No Context: {no_context}\")\n",
    "    print(f\"‚úÖ With Context: {with_context[:100]}...\")\n",
    "\n",
    "    # Practice 3: Guide Reasoning\n",
    "    print(\"\\n3Ô∏è‚É£ GUIDE REASONING\")\n",
    "    no_reasoning = \"Is this a good investment?\"\n",
    "    with_reasoning = \"\"\"Analyze whether this is a good investment opportunity. Consider these factors step by step:\n",
    "1. Financial metrics (ROI, payback period)\n",
    "2. Market conditions \n",
    "3. Risk assessment\n",
    "4. Strategic alignment\n",
    "\n",
    "Investment: Tech startup, $100K for 5% equity, projected $10M revenue in 3 years.\"\"\"\n",
    "\n",
    "    print(f\"‚ùå No Reasoning Guide: {no_reasoning}\")\n",
    "    print(f\"‚úÖ Guided Reasoning: {with_reasoning[:100]}...\")\n",
    "\n",
    "    # Practice 4: Assign Roles\n",
    "    print(\"\\n4Ô∏è‚É£ ASSIGN ROLES\")\n",
    "    no_role = \"Review this code\"\n",
    "    with_role = \"You are a senior software architect with expertise in scalable systems. Review this microservice design for performance, security, and maintainability concerns.\"\n",
    "\n",
    "    print(f\"‚ùå No Role: {no_role}\")\n",
    "    print(f\"‚úÖ With Role: {with_role[:80]}...\")\n",
    "\n",
    "    # Practice 5: Format Specification\n",
    "    print(\"\\n5Ô∏è‚É£ SPECIFY OUTPUT FORMAT\")\n",
    "    no_format = \"Analyze customer feedback\"\n",
    "    with_format = \"\"\"Analyze customer feedback and respond in this JSON format:\n",
    "{\n",
    "  \"sentiment\": \"positive/negative/neutral\",\n",
    "  \"key_themes\": [\"theme1\", \"theme2\"],\n",
    "  \"priority_score\": 1-10,\n",
    "  \"action_items\": [\"action1\", \"action2\"]\n",
    "}\n",
    "\n",
    "Feedback: \"Love the new interface but checkout process is confusing\\\"\"\"\"\n",
    "\n",
    "    print(f\"‚ùå No Format: {no_format}\")\n",
    "    print(f\"‚úÖ With Format: {with_format[:100]}...\")\n",
    "\n",
    "\n",
    "def create_prompt_template():\n",
    "    \"\"\"Create a reusable prompt template following best practices\"\"\"\n",
    "\n",
    "    template = \"\"\"\n",
    "üéØ UNIVERSAL PROMPT TEMPLATE\n",
    "\n",
    "[CONTEXT]\n",
    "You are a [ROLE] with expertise in [DOMAIN].\n",
    "\n",
    "[TASK] \n",
    "Your task is to [ACTION] based on the following [INPUT_TYPE]:\n",
    "\n",
    "[INPUT]\n",
    "{input_content}\n",
    "\n",
    "[CONSTRAINTS]\n",
    "- Format: [Specify desired format]\n",
    "- Length: [Specify length requirements]  \n",
    "- Style: [Specify tone/style]\n",
    "- Focus: [Key areas to emphasize]\n",
    "\n",
    "[OUTPUT FORMAT]\n",
    "Please structure your response as:\n",
    "1. [Section 1]\n",
    "2. [Section 2] \n",
    "3. [Section 3]\n",
    "\n",
    "[EXAMPLES] (if applicable)\n",
    "Example 1: [Show desired pattern]\n",
    "Example 2: [Show desired pattern]\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\nüõ†Ô∏è REUSABLE PROMPT TEMPLATE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(template)\n",
    "\n",
    "    # Example usage\n",
    "    example_usage = template.replace(\"[ROLE]\", \"senior data analyst\").replace(\n",
    "        \"[DOMAIN]\", \"business intelligence\"\n",
    "    ).replace(\"[ACTION]\", \"analyze sales performance\").replace(\n",
    "        \"[INPUT_TYPE]\", \"quarterly sales data\"\n",
    "    ).replace(\"{input_content}\", \"Q1: $500K, Q2: $650K, Q3: $580K, Q4: $720K\")\n",
    "\n",
    "    print(\"\\nüìù TEMPLATE USAGE EXAMPLE:\")\n",
    "    print(example_usage[:300] + \"...\")\n",
    "\n",
    "\n",
    "def performance_optimization_tips():\n",
    "    \"\"\"Show tips for optimizing prompt performance\"\"\"\n",
    "\n",
    "    print(\"\\nüöÄ PERFORMANCE OPTIMIZATION TIPS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    tips = {\n",
    "        \"Token Efficiency\": [\n",
    "            \"Remove redundant words\",\n",
    "            \"Use concise language\",\n",
    "            \"Combine similar examples\",\n",
    "            \"Optimize prompt structure\"\n",
    "        ],\n",
    "        \"Response Quality\": [\n",
    "            \"Test multiple prompt versions\",\n",
    "            \"A/B test different approaches\",\n",
    "            \"Collect user feedback\",\n",
    "            \"Iterate based on results\"\n",
    "        ],\n",
    "        \"Cost Management\": [\n",
    "            \"Set token limits\",\n",
    "            \"Cache common responses\",\n",
    "            \"Use appropriate model for task\",\n",
    "            \"Monitor usage patterns\"\n",
    "        ],\n",
    "        \"Error Handling\": [\n",
    "            \"Validate inputs\",\n",
    "            \"Handle edge cases\",\n",
    "            \"Provide fallback responses\",\n",
    "            \"Log errors for analysis\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    for category, tip_list in tips.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for tip in tip_list:\n",
    "            print(f\"  ‚úÖ {tip}\")\n",
    "\n",
    "\n",
    "# Run all best practices demonstrations\n",
    "demonstrate_best_practices()\n",
    "create_prompt_template()\n",
    "performance_optimization_tips()\n",
    "\n",
    "print(\"\\nüéâ PROMPT ENGINEERING MASTERY CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "checklist = [\n",
    "    \"‚úÖ Understand different prompting techniques\",\n",
    "    \"‚úÖ Know when to use each technique\",\n",
    "    \"‚úÖ Write clear and specific prompts\",\n",
    "    \"‚úÖ Provide context and examples\",\n",
    "    \"‚úÖ Guide reasoning with step-by-step instructions\",\n",
    "    \"‚úÖ Assign appropriate roles\",\n",
    "    \"‚úÖ Specify output formats\",\n",
    "    \"‚úÖ Test and iterate prompts\",\n",
    "    \"‚úÖ Optimize for performance and cost\",\n",
    "    \"‚úÖ Handle errors gracefully\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363dfbb",
   "metadata": {},
   "source": [
    "## üéì Summary & Next Steps\n",
    "\n",
    "### **What We've Learned**\n",
    "\n",
    "1. **Fundamental Techniques**: Single-turn, zero-shot, few-shot prompting\n",
    "2. **Advanced Methods**: Multi-turn, role-based, chain-of-thought\n",
    "3. **Complex Workflows**: Prompt chaining, self-consistency, conversational AI\n",
    "4. **Best Practices**: Clear instructions, context provision, reasoning guidance\n",
    "5. **Optimization**: Performance tuning, cost management, error handling\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- **Specificity Matters**: Clear, detailed prompts yield better results\n",
    "- **Context is King**: Provide relevant background and examples\n",
    "- **Role Assignment**: Define the AI's expertise and perspective\n",
    "- **Iterative Improvement**: Test, measure, and refine your prompts\n",
    "- **Format Control**: Specify exactly how you want responses structured\n",
    "\n",
    "### **Next Steps for Mastery**\n",
    "\n",
    "1. **Practice Daily**: Use different techniques in your work\n",
    "2. **Build Templates**: Create reusable prompt patterns\n",
    "3. **Measure Performance**: Track accuracy, cost, and user satisfaction\n",
    "4. **Stay Updated**: Follow latest research and best practices\n",
    "5. **Experiment**: Try new techniques and combinations\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "### **Documentation & Guides**\n",
    "\n",
    "- [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)\n",
    "- [Anthropic Claude Prompting Best Practices](https://docs.anthropic.com/claude/docs/prompt-engineering)\n",
    "- [Microsoft Azure OpenAI Prompt Engineering](https://docs.microsoft.com/azure/cognitive-services/openai/)\n",
    "\n",
    "### **Research Papers**\n",
    "\n",
    "- \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "- \"Few-Shot Learning with Language Models\"\n",
    "- \"Constitutional AI: Harmlessness from AI Feedback\"\n",
    "\n",
    "### **Tools & Frameworks**\n",
    "\n",
    "- **LangChain**: Framework for developing applications with LLMs\n",
    "- **Semantic Kernel**: Microsoft's SDK for AI orchestration\n",
    "- **OpenAI Playground**: Interactive prompt testing environment\n",
    "\n",
    "### **Community Resources**\n",
    "\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "- [Learn Prompting](https://learnprompting.org/)\n",
    "- [Awesome Prompt Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Practice Challenges\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. **Challenge 1**: Create a few-shot prompt for converting natural language to SQL queries\n",
    "2. **Challenge 2**: Design a role-based prompt for code review across different programming languages\n",
    "3. **Challenge 3**: Build a prompt chain for creating complete technical documentation\n",
    "4. **Challenge 4**: Develop a conversational AI assistant for a specific domain\n",
    "5. **Challenge 5**: Optimize an existing prompt for better performance and cost efficiency\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ You're now equipped with practical prompt engineering skills! Go forth and create amazing AI interactions!**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
