# Learning AI

This document outlines key concepts in AI that are essential for understanding and working with artificial intelligence systems. Each concept is briefly described to provide a foundational understanding.

## Concepts to Learn

1. **Convolutional Neural Network (CNN)**
2. **Recurrent Neural Network (RNN)**
3. **Transformer**
4. **Long Short-Term Memory (LSTM)**
5. **Attention Mechanism**
6. **Generative Adversarial Network (GAN)**
7. **Variational Autoencoder (VAE)**
8. **Reinforcement Learning**
9. **Unsupervised Learning**
10. **Supervised Learning**
11. **Semi-Supervised Learning**
12. **Transfer Learning**
13. **Self-Supervised Learning**
14. **Multi-Task Learning**
15. **Few-Shot Learning**
16. **Zero-Shot Learning**

## Please teach me about the following concepts in AI

## Convolutional Neural Network (CNN)

Convolutional Neural Networks (CNNs) are a specialized class of deep neural networks designed to process data with a grid-like topology, such as images. They consist of a series of layers that automatically learn spatial hierarchies of features:

- **Convolutional layers:** Apply multiple learnable filters (kernels) to the input to produce feature maps that capture local patterns like edges and textures.
- **Pooling layers:** Downsample feature maps (e.g., max pooling, average pooling) to reduce spatial dimensions, control overfitting, and improve translation invariance.
- **Activation functions:** Introduce non-linearity into the model, with ReLU (Rectified Linear Unit) being the most common choice.
- **Fully connected layers:** Flatten and connect features learned by convolutional layers to make final predictions (classification, regression).

CNNs learn hierarchical representations: early layers detect simple patterns, middle layers combine them into motifs, and deeper layers recognize high-level features (objects, shapes). Pioneering architectures include LeNet, AlexNet, VGG, ResNet, and Inception, and modern variants continue to improve accuracy and efficiency. Common applications of CNNs include image classification, object detection, semantic segmentation, and style transfer.

## Image Embedding

Image embeddings are compact, fixed-length vector representations that capture the semantic content of images. They are typically extracted from a pre-trained CNN by:

1. **Feature extraction:** Forward an image through a CNN (excluding its final classification layer).
2. **Vector representation:** Use activations from a chosen hidden layer (often the global pooling layer) as the embedding vector.
3. **Post-processing:** Optionally apply normalization (e.g., L2 normalization) or dimensionality reduction (e.g., PCA, t-SNE) for downstream tasks.

These embeddings enable efficient comparisons between images using distance metrics (e.g., cosine similarity, Euclidean distance). They are widely used for image retrieval, clustering, anomaly detection, and as transferable features in transfer learning workflows. Popular embedding sources include ResNet, EfficientNet, and contrastive models like CLIP.

## Text Embedding

Text embeddings are dense vector representations that capture the semantic meaning of text. They are typically generated by language models or embedding algorithms:

- **Word-level embeddings:** Models like Word2Vec or GloVe produce vectors for individual words based on co-occurrence statistics.
- **Contextual embeddings:** Transformer-based models (e.g., BERT, GPT) produce context-aware vectors for tokens or sentences by encoding the entire input sequence.
- **Sentence embeddings:** Methods like Sentence-BERT or Universal Sentence Encoder aggregate token vectors into a fixed-length vector that represents a whole sentence or paragraph.

Text embeddings enable tasks like semantic search, classification, clustering, and similarity measurement using cosine similarity or other distance metrics.

## Audio Embedding

Audio embeddings are fixed-size vectors that capture timbral and temporal characteristics of audio signals. Common steps:

1. **Feature extraction:** Convert raw audio into time-frequency representations (e.g., spectrograms, Mel-frequency cepstral coefficients - MFCCs).
2. **Learned embeddings:** Feed features into deep models (e.g., CNNs, WaveNet, or audio transformers) to produce embeddings from hidden layers.
3. **Pooling:** Apply temporal pooling (e.g., global average pooling) to obtain a single vector for the clip.

Applications include speaker recognition, audio classification, sound event detection, and music recommendation.

## Video Embedding

Video embeddings are representations that encapsulate both spatial and temporal information in video data:

- **Frame-level embeddings:** Extract image embeddings from individual frames using CNNs and aggregate them (e.g., average or attention pooling).
- **3D convolutional models:** Use 3D CNNs (e.g., C3D, I3D) to extract spatiotemporal features directly.
- **Two-stream networks:** Separate spatial (RGB frames) and motion (optical flow) streams, then fuse embeddings.
- **Transformer-based:** Apply video transformers (e.g., ViViT) to capture long-range dependencies across frames.

Use cases include action recognition, video retrieval, and video summarization.

## Graph Embedding

Graph embeddings map nodes, edges, or entire subgraphs into a continuous vector space while preserving graph structure:

- **Random-walk methods:** Techniques like DeepWalk or node2vec generate embeddings by learning from simulated walks on the graph.
- **Matrix factorization:** Methods such as Laplacian Eigenmaps factorize graph adjacency or Laplacian matrices to obtain node vectors.
- **Graph neural networks (GNNs):** Models like GraphSAGE, GAT, or GCN aggregate information from a nodeâ€™s neighborhood to learn embeddings.
- **Whole-graph embeddings:** Use graph pooling or graph-level readout functions to embed entire graphs for classification or similarity.

Graph embeddings facilitate link prediction, node classification, clustering, and recommendation tasks.
